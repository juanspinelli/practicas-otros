{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "## Eager Execution\n",
    "\n",
    "Eager execution is an experimental interface to TensorFlow that provides an imperative programming style (à la, Torch7, NumPy). When you enable eager execution, TensorFlow operations execute immediately; you do not execute a pre-constructed graph with `Session.run()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Currently we're doing this... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.]]\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[1, 1])\n",
    "m = tf.matmul(x, x)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(m, feed_dict={x: [[2.]]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Getting eager execution (warning! -- Don't try this stuff in your production systems!!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf-nightly in /Users/tarrysingh/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: enum34>=1.1.6 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: tb-nightly<1.6.0a0,>=1.5.0a0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: numpy>=1.12.1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: absl-py in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: mock>=2.0.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: backports.weakref>=1.0rc1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
      "Requirement already satisfied: setuptools in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tf-nightly)\n",
      "Requirement already satisfied: html5lib==0.9999999 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: werkzeug>=0.11.10 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: futures>=3.1.1 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: bleach==1.5.0 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
      "Requirement already satisfied: pbr>=0.11 in /Users/tarrysingh/anaconda/lib/python3.6/site-packages (from mock>=2.0.0->tf-nightly)\n"
     ]
    }
   ],
   "source": [
    "#!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed the above with the following (I hope!)\n",
    "\n",
    "```shell\n",
    "MacBook-Pro-2:~ tarrysingh$ sudo pip install tf-nightly\n",
    "Password:\n",
    "The directory '/Users/tarrysingh/Library/Caches/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
    "The directory '/Users/tarrysingh/Library/Caches/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
    "Collecting tf-nightly\n",
    "  Downloading tf_nightly-1.5.0.dev20171122-cp36-cp36m-macosx_10_11_x86_64.whl (42.4MB)\n",
    "    100% |████████████████████████████████| 42.4MB 18kB/s \n",
    "Requirement already satisfied: enum34>=1.1.6 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting mock>=2.0.0 (from tf-nightly)\n",
    "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
    "    100% |████████████████████████████████| 61kB 6.0MB/s \n",
    "Requirement already satisfied: protobuf>=3.4.0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting backports.weakref>=1.0rc1 (from tf-nightly)\n",
    "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
    "Requirement already satisfied: six>=1.10.0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: tb-nightly<1.6.0a0,>=1.5.0a0 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: numpy>=1.12.1 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Requirement already satisfied: wheel>=0.26 in ./anaconda/lib/python3.6/site-packages (from tf-nightly)\n",
    "Collecting absl-py (from tf-nightly)\n",
    "Collecting pbr>=0.11 (from mock>=2.0.0->tf-nightly)\n",
    "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
    "    100% |████████████████████████████████| 102kB 7.4MB/s \n",
    "Requirement already satisfied: setuptools in ./anaconda/lib/python3.6/site-packages (from protobuf>=3.4.0->tf-nightly)\n",
    "Requirement already satisfied: werkzeug>=0.11.10 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: markdown>=2.6.8 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: html5lib==0.9999999 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: futures>=3.1.1 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Requirement already satisfied: bleach==1.5.0 in ./anaconda/lib/python3.6/site-packages (from tb-nightly<1.6.0a0,>=1.5.0a0->tf-nightly)\n",
    "Installing collected packages: pbr, mock, backports.weakref, absl-py, tf-nightly\n",
    "Successfully installed absl-py-0.1.5 backports.weakref-1.0.post1 mock-2.0.0 pbr-3.1.1 tf-nightly-1.5.0.dev20171122\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.0-dev20171122\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are playing with fire here. Version 1.5!!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "tfe.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multiple 3 X 3 matrix\n",
    "x = tf.matmul([[1, 2, 3],\n",
    "               [4, 5, 6],\n",
    "               [7, 8, 9]\n",
    "              ],\n",
    "              [\n",
    "               [10, 11, 12],\n",
    "               [13, 14, 15],\n",
    "               [16, 17, 18],\n",
    "              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add one to each element using tf.add for broadcasting\n",
    "y = tf.add(x, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a random random matrix of, say 6 by 4 matrix\n",
    "z = tf.random_uniform([6, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul_1:0\", shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x) # without eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 84  90  96]\n",
      " [201 216 231]\n",
      " [318 342 366]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(x) # with eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 85  91  97]\n",
      " [202 217 232]\n",
      " [319 343 367]], shape=(3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(y) # with eager execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.13902152  0.30357194  0.55132675  0.29358554]\n",
      " [ 0.61529016  0.69093883  0.00179088  0.44634724]\n",
      " [ 0.97679567  0.4461503   0.75236928  0.0593257 ]\n",
      " [ 0.77715838  0.91196513  0.49345231  0.98986089]\n",
      " [ 0.67009616  0.59602785  0.50252724  0.15992355]\n",
      " [ 0.0602212   0.45736706  0.70411134  0.56758451]], shape=(6, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(z) #with eager exec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool huh!\n",
    "\n",
    "What you see is that with eager execution enabled, these ops consume and return multi-dimensional arrays as `Tensor` objects pretty much like what you see in `Torch` or `Numpy ndarray` or other imperative constructs.\n",
    "\n",
    "[**its experimental, so don't go about playing with it in production!**] there I said it again 😌\n",
    "\n",
    "These operations can also be triggered via operator overloading of the `Tensor` object.\n",
    "\n",
    "Col thing is that you can use the `+` instead of tf.add, `-` for tf.subtract etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 3.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = (tf.ones([1], dtype=tf.float32) + 1) *2 - 1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=20, shape=(1,), dtype=float32, numpy=array([ 3.], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a # here you get more info about the numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverting to and from Numpy\n",
    "\n",
    "This operations converts python objects to `Tensor` objects and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2, shape=(), dtype=int32)\n",
      "tf.Tensor(2, shape=(), dtype=int64)\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "x = tf.add(1, 1)                             # tf.Tensor with a value of 2\n",
    "y = tf.add(np.array(1), np.array(1))         # tf.Tensor w a value of 2\n",
    "z = np.multiply(x, y)                        # numpu.int64 with a value of 4\n",
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can explicitly convert using...\n",
    "\n",
    "`tf.constant` as you see next...\n",
    "\n",
    "You can also call `numpy()` method of a `Tensor` object to obtain its Numpy `ndarray` value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "np_x = np.array(2., dtype=np.float32)\n",
    "x = tf.constant(np_x)\n",
    "py_y = 3.\n",
    "y = tf.constant(py_y)\n",
    "\n",
    "# add them up + 1\n",
    "\n",
    "z = x + y + 1\n",
    "\n",
    "print(z)\n",
    "print(z.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CPU vs GPU acceleration and telling Eager Exec where to offload the computation\n",
    "\n",
    "Easiest way to do this is to enclose your computation in ith a `tf.device('/gpu:0')` block. In case you have multiple GPUs, this function might come handy `tfe.num_gpus()`, that should return the number of GPUs you have on your cluster.\n",
    "\n",
    "Here we multiply 1000 x 1000 matrices on a CPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: Took 0.2379150390625 seconds to multiply a (1000, 1000) matrix by itself 10 times\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def measure(x):\n",
    "  # The very first time a GPU is used by TensorFlow, it is initialized.\n",
    "  # So exclude the first run from timing.\n",
    "  tf.matmul(x, x)\n",
    "\n",
    "  start = time.time()\n",
    "  for i in range(10):\n",
    "    tf.matmul(x, x)\n",
    "  end = time.time()\n",
    "\n",
    "  return \"Took %s seconds to multiply a %s matrix by itself 10 times\" % (end - start, x.shape)\n",
    "\n",
    "# Run on CPU:\n",
    "with tf.device(\"/cpu:0\"):\n",
    "  print(\"CPU: %s\" % measure(tf.random_normal([1000, 1000])))\n",
    "\n",
    "# If a GPU is available, run on GPU:\n",
    "if tfe.num_gpus() > 0:\n",
    "  with tf.device(\"/gpu:0\"):\n",
    "    print(\"GPU: %s\" % measure(tf.random_normal([1000, 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error copying tensor to device: GPU:0. GPU:0 unknown device.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-101a482ca983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_gpu0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mx_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mgpu\u001b[0;34m(self, gpu_index)\u001b[0m\n\u001b[1;32m    756\u001b[0m       \u001b[0;32mas\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m     \"\"\"\n\u001b[0;32m--> 758\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"GPU:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_copy\u001b[0;34m(self, ctx, device_name)\u001b[0m\n\u001b[1;32m    713\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m       \u001b[0mnew_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error copying tensor to device: GPU:0. GPU:0 unknown device."
     ]
    }
   ],
   "source": [
    "# you can copy Tensors to different devices\n",
    "\n",
    "x = tf.random_normal([10, 10])\n",
    "\n",
    "x_gpu0 = x.gpu()\n",
    "x_cpu = x.cpu()\n",
    "\n",
    "_ = tf.matmul(x_cpu, x_cpu) # will run this operation on CPU\n",
    "_ = tf.matmul(x_gpu0, x_gpu0)# will run on your first GPU device, you can chang it to gpu1, gpu2 if you have many\n",
    "\n",
    "if tfe.num_gpus() > 1:\n",
    "    x_gpu1 = c.gpu()\n",
    "    _ = tf.matmul(x_gpu1, x_gpu1)  # will run on your second GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BTW I get an error above 👆 since I don't have a GPU on my Mac \n",
    "\n",
    "##  Automatic Differentiation with Eager Execution\n",
    "\n",
    "As per [Wikipedia](https://en.wikipedia.org/wiki/Automatic_differentiation):\n",
    "\n",
    "In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation,[1][2] is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n",
    "\n",
    "Automatic differentiation is not:\n",
    "- Symbolic differentiation, nor\n",
    "- Numerical differentiation (the method of finite differences).\n",
    "\n",
    "These classical methods run into problems: symbolic differentiation leads to inefficient code (unless carefully done) and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems, at the expense of introducing more software dependencies.\n",
    "\n",
    "<img src=\"images/AutomaticDifferentiationNutshell.png\">\n",
    "\n",
    "**Automatic Differentiation** in TensorFlow is useful when implementing several deep learning algorithms. Eager Execution provides an autograd-like API for automatic differentiation, especially for functions such as these:\n",
    "\n",
    "- `tfe.gradients_function(f)` : This returns a python function that computes the derivatives of the python function `f` w.r.t its arguments. `f` should return a scalr value. When invoeked this function returns a list of `Tensor` objects (an element for each argument of `f`)\n",
    "- `tfe.value_and_gradients_function(f)` : Just like the above function this one returns , when invoked, the value of `f` in addition to the list of derivatives of `f` w.r.t its arguments.\n",
    "\n",
    "Lets take a look how this works with an example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-df6df4e4bd34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Third order derivative.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0md3f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradients_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0md2f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0md3f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return tf.multiply(x, x)  # Or x * x\n",
    "assert 9 == f(3.).numpy()\n",
    "\n",
    "df = tfe.gradients_function(f)\n",
    "assert 6 == df(3.)[0].numpy()\n",
    "\n",
    "# Second order deriviative.\n",
    "d2f = tfe.gradients_function(lambda x : df(x)[0])\n",
    "assert 2 == d2f(3.)[0].numpy()\n",
    "\n",
    "# Third order derivative.\n",
    "d3f = tfe.gradients_function(lambda x : d2f(x)[0])\n",
    "assert 0 == d3f(3.)[0].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Still don't know why the darn Nonetype error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 70.327377\n",
      "Loss at step 0: 67.532112\n",
      "Loss at step 20: 30.218769\n",
      "Loss at step 40: 13.837612\n",
      "Loss at step 60: 6.642962\n",
      "Loss at step 80: 3.481600\n",
      "Loss at step 100: 2.091797\n",
      "Loss at step 120: 1.480482\n",
      "Loss at step 140: 1.211435\n",
      "Loss at step 160: 1.092952\n",
      "Loss at step 180: 1.040738\n",
      "Final loss: 1.018471\n",
      "W, B = 3.042722, 2.121703\n"
     ]
    }
   ],
   "source": [
    "def prediction(input, weight, bias):\n",
    "    return input * weight + bias\n",
    "\n",
    "# A toy dataset of points around 3 * x + 2\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random_normal([NUM_EXAMPLES])\n",
    "noise = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# A loss function: Mean-squared error\n",
    "def loss(weight, bias):\n",
    "    error = prediction(training_inputs, weight, bias) - training_outputs\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# Function that returns the the derivative of loss with respect to\n",
    "# weight and bias\n",
    "grad = tfe.gradients_function(loss)\n",
    "\n",
    "# Train for 200 steps (starting from some random choice for W and B, on the same\n",
    "# batch of data).\n",
    "W = 5.\n",
    "B = 10.\n",
    "learning_rate = 0.01\n",
    "print(\"Initial loss: %f\" % loss(W, B).numpy())\n",
    "for i in range(200):\n",
    "  (dW, dB) = grad(W, B)\n",
    "  W -= dW * learning_rate\n",
    "  B -= dB * learning_rate\n",
    "  if i % 20 == 0:\n",
    "    print(\"Loss at step %d: %f\" % (i, loss(W, B).numpy()))\n",
    "print(\"Final loss: %f\" % loss(W, B).numpy())\n",
    "print(\"W, B = %f, %f\" % (W.numpy(), B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing gradients\n",
    "\n",
    "One may want to define custom gradients for an operation, or for a function. This may be useful for multiple reasons, including providing a more efficient or more numerically stable gradient for a sequence of operations.\n",
    "\n",
    "For example, consider the function log(1 + e^x), which commonly occurs in the computation of cross entropy and log likelihoods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "    return tf.log( 1 + tf.exp(x))\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# works well at x = 0\n",
    "assert 0.5 == float(grad_log1pexp(0.)[0])\n",
    "\n",
    "# Returns a 'nan' at x = 100 due to numerical instability\n",
    "import math\n",
    "assert math.isnan(float(grad_log1pexp(100.)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With eager execution , define a custom gradient to simplify the gradient expression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@tfe.custom_gradient\n",
    "def log1pexp(x):\n",
    "    e = tf.exp(x)\n",
    "    def grad(dy):\n",
    "        return dy * (1 - 1 / (1 + e))\n",
    "    return tf.log(1 + e), grad\n",
    "grad_log1pexp = tfe.gradients_function(log1pexp)\n",
    "\n",
    "# when x = 0\n",
    "\n",
    "assert 0.5 == float(grad_log1pexp(0.)[0])\n",
    "\n",
    "# when x = 100, it works too\n",
    "\n",
    "assert 1.0 == float(grad_log1pexp(100.)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models : Building and Training\n",
    "\n",
    "Your computation may have several parameters to be optimized. Encapsulating them into re-usable classe and objects makes the code easier to understand and follow as compared to writing several top level functions with many arguments.\n",
    "\n",
    "Eager Execution encourages eagerly :) to use the Kersd-type \"Layer\" classes in `tf.layers` module. Furthermore, you might want to apply some more sophisticated techniques to compute parameter updates, such as those in `tf.train.Optimizer` implementations.\n",
    "\n",
    "In the next section we will walk through using `Optimizer` and `Layer` APIs to build a trainable TensorFlow graphs in an **eager execution enabled** environment.\n",
    "\n",
    "\n",
    "### Variables and Optimizers\n",
    "\n",
    "`tfe.Variable` objects store mutable `Tensor` values that can be accesed during training, making automatic differentiation easy. Particularly, parameters of a model can be encapsulated in Python classes as variable!\n",
    "\n",
    "`tfe.gradient_function(f)` introduced earlier computes the derivative of `f`, which becomes a bit clunky when `f` depends on a large number of trainable parameters.\n",
    "\n",
    "`tfe.implicit_gradients` is an alternative function with some useful properties such as:\n",
    "\n",
    "- it calculates the derivative of `f` w.r.t all the `tfe.Variable`'s used by `f`\n",
    "- when the returned function is invoked, it returns a list of (gradient value, Variable object) tuples.\n",
    "\n",
    "Representing model parameters as `Variable` objects, along the use of `tfe.implicit_gradients`, typically results in a better encapsulation. \n",
    "\n",
    "For instance, the linear regression model described above can be written in a class as following -->\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.W = tfe.Variable(5., name='weight')\n",
    "        self.B = tfe.Variable(10., name='bias')\n",
    "        \n",
    "    # Define predict function\n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 69.881783\n",
      "Loss at step 0: 67.122467\n",
      "Loss at step 20: 30.204607\n",
      "Loss at step 40: 13.911569\n",
      "Loss at step 60: 6.715141\n",
      "Loss at step 80: 3.534178\n",
      "Loss at step 100: 2.127149\n",
      "Loss at step 120: 1.504377\n",
      "Loss at step 140: 1.228563\n",
      "Loss at step 160: 1.106341\n",
      "Loss at step 180: 1.052153\n",
      "Loss at step 200: 1.028117\n",
      "Final loss: 1.028117\n",
      "W, B = 3.01984, 2.12771\n"
     ]
    }
   ],
   "source": [
    "# define loss function\n",
    "def loss(model, inputs, targets):\n",
    "    error = model.predict(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "# A baby dataset of points around 3*x + 2\n",
    "NUM_EXAMPLES     = 1000\n",
    "training_inputs  = tf.random_normal([NUM_EXAMPLES])\n",
    "noise            = tf.random_normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "\n",
    "# Define a model\n",
    "model = Model()\n",
    "# Define Derivatives\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "# Define a strategy for updating the variables based on the derivatives\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "\n",
    "# Traning the loop\n",
    "print(\"Initial loss: %f\" %\n",
    "      loss(model, training_inputs, training_outputs).numpy())\n",
    "for i in range(201):\n",
    "    optimizer.apply_gradients(grad(model, training_inputs, training_outputs))\n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step %d: %f\" % (i, loss(model, training_inputs, training_outputs).numpy()))\n",
    "# Final output\n",
    "print(\"Final loss: %f\"% loss(model, training_inputs, training_outputs). numpy())\n",
    "print(\"W, B = %s, %s\"% (model.W.numpy(), model.B.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can see benefit of implicit_gradients\n",
    "\n",
    "Using `implicit_gradients` avoids the need to provide all the trainable parameters of the model as arguments to the `loss` function.\n",
    "\n",
    "## Useing Keras and the Layers API\n",
    "\n",
    "Let's see how it works with Keras, the popular and very simple to use API for defining model structures in the Eager Execution module of TensorFlow. You will notice that the `tf.keras.layers` module provides a set of building blocks for models and is implemented using `tf.keras.Layer` subclasses in the `tf.layers` module.\n",
    "\n",
    "Let's try it out!\n",
    "\n",
    "**NOTE to self**: (Planned test on CPU/GPU for 25th Nov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        self.layer = tf.layers.Dense(1)\n",
    "        \n",
    "    def predict(self, inputs):\n",
    "        return self.layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: What does tf.layers API do\n",
    "\n",
    "Next, you'll see how tf.layers API makes it easy to define sophisticated models. Well, we're used to the beauty of Keras already, lets see how it plays out in TensorFlow 1.4.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-43-9ff01676a66a>:35: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "Step 0: Loss on training set : 2.256627\n",
      "Step 100: Loss on training set : 0.399705\n",
      "Step 200: Loss on training set : 0.214346\n",
      "Step 300: Loss on training set : 0.211841\n",
      "Step 400: Loss on training set : 0.119034\n",
      "Step 500: Loss on training set : 0.085323\n",
      "Step 600: Loss on training set : 0.231022\n",
      "Step 700: Loss on training set : 0.061058\n",
      "Step 800: Loss on training set : 0.196906\n",
      "Step 900: Loss on training set : 0.156831\n",
      "Step 1000: Loss on training set : 0.091805\n",
      "Step 1100: Loss on training set : 0.024188\n",
      "Step 1200: Loss on training set : 0.092624\n",
      "Step 1300: Loss on training set : 0.019488\n",
      "Step 1400: Loss on training set : 0.064160\n",
      "Step 1500: Loss on training set : 0.044069\n",
      "Step 1600: Loss on training set : 0.088605\n",
      "Step 1700: Loss on training set : 0.004956\n",
      "Step 1800: Loss on training set : 0.044108\n",
      "Step 1900: Loss on training set : 0.050574\n",
      "Step 2000: Loss on training set : 0.013534\n",
      "Step 2100: Loss on training set : 0.068764\n",
      "Step 2200: Loss on training set : 0.061247\n",
      "Step 2300: Loss on training set : 0.134102\n",
      "Step 2400: Loss on training set : 0.002189\n",
      "Step 2500: Loss on training set : 0.002621\n",
      "Step 2600: Loss on training set : 0.084751\n",
      "Step 2700: Loss on training set : 0.073403\n",
      "Step 2800: Loss on training set : 0.034124\n",
      "Step 2900: Loss on training set : 0.068016\n",
      "Step 3000: Loss on training set : 0.026844\n",
      "Step 3100: Loss on training set : 0.008452\n",
      "Step 3200: Loss on training set : 0.052670\n",
      "Step 3300: Loss on training set : 0.095155\n",
      "Step 3400: Loss on training set : 0.019506\n",
      "Step 3500: Loss on training set : 0.015484\n",
      "Step 3600: Loss on training set : 0.007086\n",
      "Step 3700: Loss on training set : 0.045831\n",
      "Step 3800: Loss on training set : 0.058367\n",
      "Step 3900: Loss on training set : 0.042314\n",
      "Step 4000: Loss on training set : 0.097755\n",
      "Step 4100: Loss on training set : 0.030364\n",
      "Step 4200: Loss on training set : 0.019168\n",
      "Step 4300: Loss on training set : 0.019495\n",
      "Step 4400: Loss on training set : 0.030664\n",
      "Step 4500: Loss on training set : 0.033287\n",
      "Step 4600: Loss on training set : 0.105648\n",
      "Step 4700: Loss on training set : 0.009519\n",
      "Step 4800: Loss on training set : 0.009824\n",
      "Step 4900: Loss on training set : 0.002434\n",
      "Step 5000: Loss on training set : 0.002829\n",
      "Step 5100: Loss on training set : 0.003514\n",
      "Step 5200: Loss on training set : 0.110153\n",
      "Step 5300: Loss on training set : 0.006671\n",
      "Step 5400: Loss on training set : 0.022709\n",
      "Step 5500: Loss on training set : 0.004206\n",
      "Step 5600: Loss on training set : 0.021019\n",
      "Step 5700: Loss on training set : 0.010719\n",
      "Step 5800: Loss on training set : 0.004459\n",
      "Step 5900: Loss on training set : 0.025301\n",
      "Step 6000: Loss on training set : 0.004324\n",
      "Step 6100: Loss on training set : 0.005369\n",
      "Step 6200: Loss on training set : 0.047895\n",
      "Step 6300: Loss on training set : 0.037086\n",
      "Step 6400: Loss on training set : 0.054543\n",
      "Step 6500: Loss on training set : 0.048353\n",
      "Step 6600: Loss on training set : 0.010139\n",
      "Step 6700: Loss on training set : 0.023344\n",
      "Step 6800: Loss on training set : 0.005961\n",
      "Step 6900: Loss on training set : 0.027603\n",
      "Step 7000: Loss on training set : 0.001533\n",
      "Step 7100: Loss on training set : 0.031711\n",
      "Step 7200: Loss on training set : 0.005685\n",
      "Step 7300: Loss on training set : 0.003476\n",
      "Step 7400: Loss on training set : 0.004660\n",
      "Step 7500: Loss on training set : 0.065238\n",
      "Step 7600: Loss on training set : 0.008798\n",
      "Step 7700: Loss on training set : 0.024648\n",
      "Step 7800: Loss on training set : 0.025574\n",
      "Step 7900: Loss on training set : 0.020188\n",
      "Step 8000: Loss on training set : 0.018603\n",
      "Step 8100: Loss on training set : 0.002296\n",
      "Step 8200: Loss on training set : 0.051427\n",
      "Step 8300: Loss on training set : 0.005059\n",
      "Step 8400: Loss on training set : 0.003244\n",
      "Step 8500: Loss on training set : 0.003312\n",
      "Step 8600: Loss on training set : 0.018318\n",
      "Step 8700: Loss on training set : 0.020676\n",
      "Step 8800: Loss on training set : 0.004160\n",
      "Step 8900: Loss on training set : 0.004872\n",
      "Step 9000: Loss on training set : 0.001758\n",
      "Step 9100: Loss on training set : 0.002270\n",
      "Step 9200: Loss on training set : 0.000740\n",
      "Step 9300: Loss on training set : 0.002610\n",
      "Step 9400: Loss on training set : 0.003372\n",
      "Step 9500: Loss on training set : 0.002058\n",
      "Step 9600: Loss on training set : 0.000575\n",
      "Step 9700: Loss on training set : 0.008625\n",
      "Step 9800: Loss on training set : 0.010724\n",
      "Step 9900: Loss on training set : 0.025758\n",
      "Step 10000: Loss on training set : 0.006348\n",
      "Step 10100: Loss on training set : 0.002577\n",
      "Step 10200: Loss on training set : 0.012478\n",
      "Step 10300: Loss on training set : 0.006424\n",
      "Step 10400: Loss on training set : 0.007878\n",
      "Step 10500: Loss on training set : 0.010238\n",
      "Step 10600: Loss on training set : 0.011099\n",
      "Step 10700: Loss on training set : 0.000476\n",
      "Step 10800: Loss on training set : 0.000640\n",
      "Step 10900: Loss on training set : 0.001908\n",
      "Step 11000: Loss on training set : 0.002969\n",
      "Step 11100: Loss on training set : 0.002400\n",
      "Step 11200: Loss on training set : 0.005269\n",
      "Step 11300: Loss on training set : 0.018153\n",
      "Step 11400: Loss on training set : 0.002701\n",
      "Step 11500: Loss on training set : 0.016919\n",
      "Step 11600: Loss on training set : 0.001562\n",
      "Step 11700: Loss on training set : 0.001536\n",
      "Step 11800: Loss on training set : 0.001691\n",
      "Step 11900: Loss on training set : 0.001171\n",
      "Step 12000: Loss on training set : 0.007421\n",
      "Step 12100: Loss on training set : 0.002470\n",
      "Step 12200: Loss on training set : 0.002226\n",
      "Step 12300: Loss on training set : 0.025579\n",
      "Step 12400: Loss on training set : 0.007481\n",
      "Step 12500: Loss on training set : 0.011208\n",
      "Step 12600: Loss on training set : 0.005309\n",
      "Step 12700: Loss on training set : 0.001684\n",
      "Step 12800: Loss on training set : 0.010187\n",
      "Step 12900: Loss on training set : 0.001179\n",
      "Step 13000: Loss on training set : 0.012636\n",
      "Step 13100: Loss on training set : 0.004405\n",
      "Step 13200: Loss on training set : 0.000556\n",
      "Step 13300: Loss on training set : 0.001598\n",
      "Step 13400: Loss on training set : 0.001437\n",
      "Step 13500: Loss on training set : 0.001434\n",
      "Step 13600: Loss on training set : 0.037155\n",
      "Step 13700: Loss on training set : 0.001473\n",
      "Step 13800: Loss on training set : 0.016352\n",
      "Step 13900: Loss on training set : 0.000089\n",
      "Step 14000: Loss on training set : 0.010603\n",
      "Step 14100: Loss on training set : 0.000232\n",
      "Step 14200: Loss on training set : 0.000834\n",
      "Step 14300: Loss on training set : 0.000423\n",
      "Step 14400: Loss on training set : 0.000145\n",
      "Step 14500: Loss on training set : 0.000219\n",
      "Step 14600: Loss on training set : 0.012533\n",
      "Step 14700: Loss on training set : 0.000458\n",
      "Step 14800: Loss on training set : 0.000817\n",
      "Step 14900: Loss on training set : 0.002362\n",
      "Step 15000: Loss on training set : 0.000349\n",
      "Step 15100: Loss on training set : 0.012360\n",
      "Step 15200: Loss on training set : 0.021442\n",
      "Step 15300: Loss on training set : 0.007397\n",
      "Step 15400: Loss on training set : 0.000550\n",
      "Step 15500: Loss on training set : 0.001426\n",
      "Step 15600: Loss on training set : 0.000598\n",
      "Step 15700: Loss on training set : 0.033891\n",
      "Step 15800: Loss on training set : 0.003439\n",
      "Step 15900: Loss on training set : 0.001901\n",
      "Step 16000: Loss on training set : 0.001585\n",
      "Step 16100: Loss on training set : 0.008619\n",
      "Step 16200: Loss on training set : 0.000461\n",
      "Step 16300: Loss on training set : 0.001010\n",
      "Step 16400: Loss on training set : 0.007105\n",
      "Step 16500: Loss on training set : 0.000262\n",
      "Step 16600: Loss on training set : 0.001249\n",
      "Step 16700: Loss on training set : 0.000257\n",
      "Step 16800: Loss on training set : 0.000844\n",
      "Step 16900: Loss on training set : 0.014114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 17000: Loss on training set : 0.001968\n",
      "Step 17100: Loss on training set : 0.000176\n",
      "Step 17200: Loss on training set : 0.001083\n",
      "Step 17300: Loss on training set : 0.001023\n",
      "Step 17400: Loss on training set : 0.002160\n",
      "Step 17500: Loss on training set : 0.004066\n",
      "Step 17600: Loss on training set : 0.002666\n",
      "Step 17700: Loss on training set : 0.001996\n",
      "Step 17800: Loss on training set : 0.001733\n",
      "Step 17900: Loss on training set : 0.000166\n",
      "Step 18000: Loss on training set : 0.000297\n",
      "Step 18100: Loss on training set : 0.000313\n",
      "Step 18200: Loss on training set : 0.005266\n",
      "Step 18300: Loss on training set : 0.000845\n",
      "Step 18400: Loss on training set : 0.002949\n",
      "Step 18500: Loss on training set : 0.000892\n",
      "Step 18600: Loss on training set : 0.003773\n",
      "Step 18700: Loss on training set : 0.003751\n",
      "Step 18800: Loss on training set : 0.000007\n",
      "Step 18900: Loss on training set : 0.000056\n",
      "Step 19000: Loss on training set : 0.000011\n",
      "Step 19100: Loss on training set : 0.000058\n",
      "Step 19200: Loss on training set : 0.001537\n",
      "Step 19300: Loss on training set : 0.023998\n",
      "Step 19400: Loss on training set : 0.000132\n",
      "Step 19500: Loss on training set : 0.000506\n",
      "Step 19600: Loss on training set : 0.000474\n",
      "Step 19700: Loss on training set : 0.000561\n",
      "Step 19800: Loss on training set : 0.000205\n",
      "Step 19900: Loss on training set : 0.001069\n",
      "Step 20000: Loss on training set : 0.000826\n",
      "Loss on test set: 0.024786\n"
     ]
    }
   ],
   "source": [
    "class MNISTModel(object):\n",
    "  def __init__(self, data_format):\n",
    "    # 'channels_first' is typically faster on GPUs\n",
    "    # while 'channels_last' is typically faster on CPUs.\n",
    "    # See: https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "    if data_format == 'channels_first':\n",
    "      self._input_shape = [-1, 1, 28, 28]\n",
    "    else:\n",
    "      self._input_shape = [-1, 28, 28, 1]\n",
    "    self.conv1 = tf.layers.Conv2D(32, 5,\n",
    "                                  padding='same',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  data_format=data_format)\n",
    "    self.max_pool2d = tf.layers.MaxPooling2D(\n",
    "        (2, 2), (2, 2), padding='same', data_format=data_format)\n",
    "    self.conv2 = tf.layers.Conv2D(64, 5,\n",
    "                                  padding='same',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  data_format=data_format)\n",
    "    self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)\n",
    "    self.dropout = tf.layers.Dropout(0.5)\n",
    "    self.dense2 = tf.layers.Dense(10)\n",
    "\n",
    "  def predict(self, inputs):\n",
    "    x = tf.reshape(inputs, self._input_shape)\n",
    "    x = self.max_pool2d(self.conv1(x))\n",
    "    x = self.max_pool2d(self.conv2(x))\n",
    "    x = tf.layers.flatten(x)\n",
    "    x = self.dropout(self.dense1(x))\n",
    "    return self.dense2(x)\n",
    "\n",
    "def loss(model, inputs, targets):\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "          logits=model.predict(inputs), labels=targets))\n",
    "\n",
    "\n",
    "# Load the training and validation data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"./mnist_data\", one_hot=True)\n",
    "\n",
    "# Train\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n",
    "model = MNISTModel('channels_first' if tfe.num_gpus() else 'channels_last')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "for i in range(20001):\n",
    "  with tf.device(device):\n",
    "    (inputs, targets) = data.train.next_batch(50)\n",
    "    optimizer.apply_gradients(grad(model, inputs, targets))\n",
    "    if i % 100 == 0:\n",
    "      print(\"Step %d: Loss on training set : %f\" %\n",
    "            (i, loss(model, inputs, targets).numpy()))\n",
    "print(\"Loss on test set: %f\" % loss(model, data.test.images, data.test.labels).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function softmax_cross_entropy_with_logits_v2 in module tensorflow.python.ops.nn_ops:\n",
      "\n",
      "softmax_cross_entropy_with_logits_v2(_sentinel=None, labels=None, logits=None, dim=-1, name=None)\n",
      "    Computes softmax cross entropy between `logits` and `labels`.\n",
      "    \n",
      "    Measures the probability error in discrete classification tasks in which the\n",
      "    classes are mutually exclusive (each entry is in exactly one class).  For\n",
      "    example, each CIFAR-10 image is labeled with one and only one label: an image\n",
      "    can be a dog or a truck, but not both.\n",
      "    \n",
      "    **NOTE:**  While the classes are mutually exclusive, their probabilities\n",
      "    need not be.  All that is required is that each row of `labels` is\n",
      "    a valid probability distribution.  If they are not, the computation of the\n",
      "    gradient will be incorrect.\n",
      "    \n",
      "    If using exclusive `labels` (wherein one and only\n",
      "    one class is true at a time), see `sparse_softmax_cross_entropy_with_logits`.\n",
      "    \n",
      "    **WARNING:** This op expects unscaled logits, since it performs a `softmax`\n",
      "    on `logits` internally for efficiency.  Do not call this op with the\n",
      "    output of `softmax`, as it will produce incorrect results.\n",
      "    \n",
      "    `logits` and `labels` must have the same shape, e.g.\n",
      "    `[batch_size, num_classes]` and the same dtype (either `float16`, `float32`,\n",
      "    or `float64`).\n",
      "    \n",
      "    Backpropagation will happen into both `logits` and `labels`.  To disallow\n",
      "    backpropagation into `labels`, pass label tensors through a `stop_gradients`\n",
      "    before feeding it to this function.\n",
      "    \n",
      "    **Note that to avoid confusion, it is required to pass only named arguments to\n",
      "    this function.**\n",
      "    \n",
      "    Args:\n",
      "      _sentinel: Used to prevent positional parameters. Internal, do not use.\n",
      "      labels: Each row `labels[i]` must be a valid probability distribution.\n",
      "      logits: Unscaled log probabilities.\n",
      "      dim: The class dimension. Defaulted to -1 which is the last dimension.\n",
      "      name: A name for the operation (optional).\n",
      "    \n",
      "    Returns:\n",
      "      A 1-D `Tensor` of length `batch_size` of the same type as `logits` with the\n",
      "      softmax cross entropy loss.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.nn.softmax_cross_entropy_with_logits_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running with  tf.nn.softmax_cross_entropy_with_logits_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: Loss on training set : 2.259213\n",
      "Step 100: Loss on training set : 0.476452\n",
      "Step 200: Loss on training set : 0.327814\n",
      "Step 300: Loss on training set : 0.215437\n",
      "Step 400: Loss on training set : 0.047517\n",
      "Step 500: Loss on training set : 0.210963\n",
      "Step 600: Loss on training set : 0.093315\n",
      "Step 700: Loss on training set : 0.161099\n",
      "Step 800: Loss on training set : 0.121512\n",
      "Step 900: Loss on training set : 0.098316\n",
      "Step 1000: Loss on training set : 0.141473\n",
      "Step 1100: Loss on training set : 0.055444\n",
      "Step 1200: Loss on training set : 0.014643\n",
      "Step 1300: Loss on training set : 0.066799\n",
      "Step 1400: Loss on training set : 0.021380\n",
      "Step 1500: Loss on training set : 0.079113\n",
      "Step 1600: Loss on training set : 0.058016\n",
      "Step 1700: Loss on training set : 0.114372\n",
      "Step 1800: Loss on training set : 0.095279\n",
      "Step 1900: Loss on training set : 0.111898\n",
      "Step 2000: Loss on training set : 0.098333\n",
      "Loss on test set: 0.057018\n"
     ]
    }
   ],
   "source": [
    "class MNISTModel(object):\n",
    "  def __init__(self, data_format):\n",
    "    # 'channels_first' is typically faster on GPUs\n",
    "    # while 'channels_last' is typically faster on CPUs.\n",
    "    # See: https://www.tensorflow.org/performance/performance_guide#data_formats\n",
    "    if data_format == 'channels_first':\n",
    "      self._input_shape = [-1, 1, 28, 28]\n",
    "    else:\n",
    "      self._input_shape = [-1, 28, 28, 1]\n",
    "    self.conv1 = tf.layers.Conv2D(32, 5,\n",
    "                                  padding='same',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  data_format=data_format)\n",
    "    self.max_pool2d = tf.layers.MaxPooling2D(\n",
    "        (2, 2), (2, 2), padding='same', data_format=data_format)\n",
    "    self.conv2 = tf.layers.Conv2D(64, 5,\n",
    "                                  padding='same',\n",
    "                                  activation=tf.nn.relu,\n",
    "                                  data_format=data_format)\n",
    "    self.dense1 = tf.layers.Dense(1024, activation=tf.nn.relu)\n",
    "    self.dropout = tf.layers.Dropout(0.5)\n",
    "    self.dense2 = tf.layers.Dense(10)\n",
    "\n",
    "  def predict(self, inputs):\n",
    "    x = tf.reshape(inputs, self._input_shape)\n",
    "    x = self.max_pool2d(self.conv1(x))\n",
    "    x = self.max_pool2d(self.conv2(x))\n",
    "    x = tf.layers.flatten(x)\n",
    "    x = self.dropout(self.dense1(x))\n",
    "    return self.dense2(x)\n",
    "\n",
    "def loss(model, inputs, targets):\n",
    "  return tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "          logits=model.predict(inputs), labels=targets))\n",
    "\n",
    "\n",
    "# Load the training and validation data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "data = input_data.read_data_sets(\"./mnist_data\", one_hot=True)\n",
    "\n",
    "# Train\n",
    "device = \"gpu:0\" if tfe.num_gpus() else \"cpu:0\"\n",
    "model = MNISTModel('channels_first' if tfe.num_gpus() else 'channels_last')\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "grad = tfe.implicit_gradients(loss)\n",
    "for i in range(2001):\n",
    "  with tf.device(device):\n",
    "    (inputs, targets) = data.train.next_batch(50)\n",
    "    optimizer.apply_gradients(grad(model, inputs, targets))\n",
    "    if i % 100 == 0:\n",
    "      print(\"Step %d: Loss on training set : %f\" %\n",
    "            (i, loss(model, inputs, targets).numpy()))\n",
    "print(\"Loss on test set: %f\" % loss(model, data.test.images, data.test.labels).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing trained variables\n",
    "\n",
    "TensorFlow Eager Execution Variables `tfe.Variables` provide a way to represent shared, persistent state of the model you make. The `tfe.Saver` class -- which is a thin wrapper over the `tf.train.Saver` class provides means to save and restore variables to and from checkpoints.\n",
    "\n",
    "As an examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/ckpt\n"
     ]
    }
   ],
   "source": [
    "# We create variables\n",
    "x = tfe.Variable(10., name='x')\n",
    "y = tfe.Variable(5., name='y')\n",
    "\n",
    "# We create tfe.Saver instead\n",
    "saver = tfe.Saver([x, y])\n",
    "\n",
    "# We assign new values to variable and save\n",
    "x.assign(2.)\n",
    "saver.save('/tmp/ckpt')\n",
    "\n",
    "# Change the var after saving\n",
    "x.assign(11.)\n",
    "assert 16. == (x + y).numpy()\n",
    "\n",
    "# Restore value in the checkpoint\n",
    "saver.restore('/tmp/ckpt')\n",
    "\n",
    "assert 7. == (x + y).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tfe.Network\n",
    "\n",
    "We tend to make it a best practice and organize our model using classes, like the famous `MNISTModel` we use everyday. Inheriting from the `tfe.Network` class provides nice and easy things like keeping a track of all models varialbes and methods to save and restore checkpoints.\n",
    "\n",
    "Subclasses"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
