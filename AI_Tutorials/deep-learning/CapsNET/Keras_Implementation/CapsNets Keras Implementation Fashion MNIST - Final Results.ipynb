{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import layers, models, optimizers\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers, layers\n",
    "import fashion_mnist as fashion\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import callbacks\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import combine_images\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elements of Capsule Layers\n",
    "\n",
    "Definitions will emerge here for all parts of Capsule Layer\n",
    "\n",
    "- Class Length\n",
    "- Class Mask\n",
    "- Squashing Function\n",
    "- Class Capsule Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    \"\"\"\n",
    "    Compute the length of vectors. This is used to compute a Tensor that has the same shape with y_true in margin_loss\n",
    "    inputs: shape=[dim_1, ..., dim_{n-1}, dim_n]\n",
    "    output: shape=[dim_1, ..., dim_{n-1}]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Mask(layers.Layer):\n",
    "    \"\"\"\n",
    "    Mask a Tensor with shape=[None, d1, d2] by the max value in axis=1.\n",
    "    Output shape: [None, d2]\n",
    "    \"\"\"\n",
    "    def call(self, inputs, **kwargs):\n",
    "        # use true label to select target capsule, shape=[batch_size, num_capsule]\n",
    "        if type(inputs) is list:  # true label is provided with shape = [batch_size, n_classes], i.e. one-hot code.\n",
    "            assert len(inputs) == 2\n",
    "            inputs, mask = inputs\n",
    "        else:  # if no true label, mask by the max length of vectors of capsules\n",
    "            x = inputs\n",
    "            # Enlarge the range of values in x to make max(new_x)=1 and others < 0\n",
    "            x = (x - K.max(x, 1, True)) / K.epsilon() + 1\n",
    "            mask = K.clip(x, 0, 1)  # the max value in x clipped to 1 and other to 0\n",
    "\n",
    "        # masked inputs, shape = [batch_size, dim_vector]\n",
    "        inputs_masked = K.batch_dot(inputs, mask, [1, 1])\n",
    "        return inputs_masked\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if type(input_shape[0]) is tuple:  # true label provided\n",
    "            return tuple([None, input_shape[0][-1]])\n",
    "        else:\n",
    "            return tuple([None, input_shape[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"\n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors\n",
    "    \"\"\"\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the \n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    \n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "                                 initializer=self.kernel_initializer,\n",
    "                                 name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\" \n",
    "        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        \n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            if i != 1:\n",
    "                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n",
    "                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        # End: routing algorithm V2, static ------------------------------------------------------------#\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding):\n",
    "    \"\"\"\n",
    "    Apply Conv2D `n_channels` times and concatenate all capsules\n",
    "    :param inputs: 4D tensor, shape=[None, width, height, channels]\n",
    "    :param dim_vector: the dim of the output vector of capsule\n",
    "    :param n_channels: the number of types of capsules\n",
    "    :return: output tensor, shape=[None, num_capsule, dim_vector]\n",
    "    \"\"\"\n",
    "    output = layers.Conv2D(filters=dim_vector*n_channels, kernel_size=kernel_size, strides=strides, padding=padding,\n",
    "                           name='primarycap_conv2d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Capsule Network Section\n",
    "\n",
    "More details will emerge here on definition, choices, architecture and eventual code explanations.\n",
    "\n",
    "- Function Convolutional Block\n",
    "- Two versions of CapsNet (details to come later)\n",
    "- Function Margin Loss\n",
    "- Function to Train and to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    \"\"\"\n",
    "    A Capsule Network on MNIST.\n",
    "    :param input_shape: data shape, 3d, [width, height, channels]\n",
    "    :param n_class: number of classes\n",
    "    :param num_routing: number of routing iterations\n",
    "    :return: A Keras Model with 2 inputs and 2 outputs\n",
    "    \"\"\"\n",
    "    x = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Layer 1: Just a conventional Conv2D layer\n",
    "    conv1 = layers.Conv2D(filters=256, kernel_size=9, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "\n",
    "    # Layer 2: Conv2D layer with `squash` activation, then reshape to [None, num_capsule, dim_vector]\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=32, kernel_size=9, strides=2, padding='valid')\n",
    "\n",
    "    # Layer 3: Capsule layer. Routing algorithm works here.\n",
    "    digitcaps = CapsuleLayer(num_capsule=n_class, dim_vector=16, num_routing=num_routing, name='digitcaps')(primarycaps)\n",
    "\n",
    "    # Layer 4: This is an auxiliary layer to replace each capsule with its length. Just to match the true label's shape.\n",
    "    # If using tensorflow, this will not be necessary. :)\n",
    "    out_caps = Length(name='out_caps')(digitcaps)\n",
    "\n",
    "    # Decoder network.\n",
    "    y = layers.Input(shape=(n_class,))\n",
    "    masked = Mask()([digitcaps, y])  # The true label is used to mask the output of capsule layer.\n",
    "    x_recon = layers.Dense(512, activation='relu')(masked)\n",
    "    x_recon = layers.Dense(1024, activation='relu')(x_recon)\n",
    "    x_recon = layers.Dense(np.prod(input_shape), activation='sigmoid')(x_recon)\n",
    "    x_recon = layers.Reshape(target_shape=input_shape, name='out_recon')(x_recon)\n",
    "\n",
    "    # two-input-two-output keras Model\n",
    "    return models.Model([x, y], [out_caps, x_recon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Margin loss for Eq.(4). When y_true[i, :] contains not just one `1`, this loss should work too. Not test it.\n",
    "    :param y_true: [None, n_classes]\n",
    "    :param y_pred: [None, num_capsule]\n",
    "    :return: a scalar loss value.\n",
    "    \"\"\"\n",
    "    L = y_true * K.square(K.maximum(0., 0.9 - y_pred)) + \\\n",
    "        0.5 * (1 - y_true) * K.square(K.maximum(0., y_pred - 0.1))\n",
    "\n",
    "    return K.mean(K.sum(L, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 28, 28, 1)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                   (None, 20, 20, 256)   20992       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "primarycap_conv2d (Conv2D)       (None, 6, 6, 256)     5308672     conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "primarycap_reshape (Reshape)     (None, 1152, 8)       0           primarycap_conv2d[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "primarycap_squash (Lambda)       (None, 1152, 8)       0           primarycap_reshape[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "digitcaps (CapsuleLayer)         (None, 10, 16)        1486080     primarycap_squash[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "input_2 (InputLayer)             (None, 10)            0                                            \n",
      "____________________________________________________________________________________________________\n",
      "mask_1 (Mask)                    (None, 16)            0           digitcaps[0][0]                  \n",
      "                                                                   input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           8704        mask_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1024)          525312      dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 784)           803600      dense_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "out_caps (Length)                (None, 10)            0           digitcaps[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "out_recon (Reshape)              (None, 28, 28, 1)     0           dense_3[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 8,153,360\n",
      "Trainable params: 8,141,840\n",
      "Non-trainable params: 11,520\n",
      "____________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.2356 - out_caps_loss: 0.2119 - out_recon_loss: 0.0605 - out_caps_acc: 0.7042Epoch 00000: val_loss improved from inf to 0.15080, saving model to ./result/weights-00.h5\n",
      "468/468 [==============================] - 576s - loss: 0.2354 - out_caps_loss: 0.2117 - out_recon_loss: 0.0604 - out_caps_acc: 0.7044 - val_loss: 0.1508 - val_out_caps_loss: 0.1351 - val_out_recon_loss: 0.0402 - val_out_caps_acc: 0.8176\n",
      "Epoch 2/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1478 - out_caps_loss: 0.1326 - out_recon_loss: 0.0389 - out_caps_acc: 0.8195Epoch 00001: val_loss improved from 0.15080 to 0.12875, saving model to ./result/weights-01.h5\n",
      "468/468 [==============================] - 579s - loss: 0.1479 - out_caps_loss: 0.1327 - out_recon_loss: 0.0389 - out_caps_acc: 0.8194 - val_loss: 0.1287 - val_out_caps_loss: 0.1156 - val_out_recon_loss: 0.0337 - val_out_caps_acc: 0.8466\n",
      "Epoch 3/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1303 - out_caps_loss: 0.1169 - out_recon_loss: 0.0343 - out_caps_acc: 0.8424Epoch 00002: val_loss improved from 0.12875 to 0.12057, saving model to ./result/weights-02.h5\n",
      "468/468 [==============================] - 579s - loss: 0.1303 - out_caps_loss: 0.1169 - out_recon_loss: 0.0343 - out_caps_acc: 0.8424 - val_loss: 0.1206 - val_out_caps_loss: 0.1081 - val_out_recon_loss: 0.0318 - val_out_caps_acc: 0.8530\n",
      "Epoch 4/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1209 - out_caps_loss: 0.1083 - out_recon_loss: 0.0321 - out_caps_acc: 0.8553Epoch 00003: val_loss improved from 0.12057 to 0.11352, saving model to ./result/weights-03.h5\n",
      "468/468 [==============================] - 569s - loss: 0.1209 - out_caps_loss: 0.1083 - out_recon_loss: 0.0321 - out_caps_acc: 0.8552 - val_loss: 0.1135 - val_out_caps_loss: 0.1017 - val_out_recon_loss: 0.0301 - val_out_caps_acc: 0.8630\n",
      "Epoch 5/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1143 - out_caps_loss: 0.1022 - out_recon_loss: 0.0309 - out_caps_acc: 0.8625Epoch 00004: val_loss improved from 0.11352 to 0.10815, saving model to ./result/weights-04.h5\n",
      "468/468 [==============================] - 562s - loss: 0.1144 - out_caps_loss: 0.1023 - out_recon_loss: 0.0309 - out_caps_acc: 0.8624 - val_loss: 0.1081 - val_out_caps_loss: 0.0968 - val_out_recon_loss: 0.0288 - val_out_caps_acc: 0.8653\n",
      "Epoch 6/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1092 - out_caps_loss: 0.0976 - out_recon_loss: 0.0296 - out_caps_acc: 0.8689Epoch 00005: val_loss improved from 0.10815 to 0.10253, saving model to ./result/weights-05.h5\n",
      "468/468 [==============================] - 562s - loss: 0.1092 - out_caps_loss: 0.0976 - out_recon_loss: 0.0296 - out_caps_acc: 0.8690 - val_loss: 0.1025 - val_out_caps_loss: 0.0913 - val_out_recon_loss: 0.0286 - val_out_caps_acc: 0.8751\n",
      "Epoch 7/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1053 - out_caps_loss: 0.0940 - out_recon_loss: 0.0286 - out_caps_acc: 0.8741Epoch 00006: val_loss improved from 0.10253 to 0.09909, saving model to ./result/weights-06.h5\n",
      "468/468 [==============================] - 562s - loss: 0.1052 - out_caps_loss: 0.0940 - out_recon_loss: 0.0286 - out_caps_acc: 0.8741 - val_loss: 0.0991 - val_out_caps_loss: 0.0882 - val_out_recon_loss: 0.0277 - val_out_caps_acc: 0.8816\n",
      "Epoch 8/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.1008 - out_caps_loss: 0.0899 - out_recon_loss: 0.0278 - out_caps_acc: 0.8785Epoch 00007: val_loss improved from 0.09909 to 0.09342, saving model to ./result/weights-07.h5\n",
      "468/468 [==============================] - 562s - loss: 0.1008 - out_caps_loss: 0.0900 - out_recon_loss: 0.0278 - out_caps_acc: 0.8785 - val_loss: 0.0934 - val_out_caps_loss: 0.0831 - val_out_recon_loss: 0.0263 - val_out_caps_acc: 0.8877\n",
      "Epoch 9/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0974 - out_caps_loss: 0.0867 - out_recon_loss: 0.0273 - out_caps_acc: 0.8849Epoch 00008: val_loss improved from 0.09342 to 0.09217, saving model to ./result/weights-08.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0974 - out_caps_loss: 0.0867 - out_recon_loss: 0.0273 - out_caps_acc: 0.8848 - val_loss: 0.0922 - val_out_caps_loss: 0.0818 - val_out_recon_loss: 0.0263 - val_out_caps_acc: 0.8893\n",
      "Epoch 10/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0950 - out_caps_loss: 0.0845 - out_recon_loss: 0.0267 - out_caps_acc: 0.8875Epoch 00009: val_loss improved from 0.09217 to 0.09079, saving model to ./result/weights-09.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0950 - out_caps_loss: 0.0845 - out_recon_loss: 0.0267 - out_caps_acc: 0.8875 - val_loss: 0.0908 - val_out_caps_loss: 0.0807 - val_out_recon_loss: 0.0257 - val_out_caps_acc: 0.8894\n",
      "Epoch 11/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0927 - out_caps_loss: 0.0824 - out_recon_loss: 0.0263 - out_caps_acc: 0.8899Epoch 00010: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 562s - loss: 0.0927 - out_caps_loss: 0.0824 - out_recon_loss: 0.0263 - out_caps_acc: 0.8900 - val_loss: 0.0930 - val_out_caps_loss: 0.0827 - val_out_recon_loss: 0.0262 - val_out_caps_acc: 0.8876\n",
      "Epoch 12/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0906 - out_caps_loss: 0.0805 - out_recon_loss: 0.0257 - out_caps_acc: 0.8925Epoch 00011: val_loss improved from 0.09079 to 0.08597, saving model to ./result/weights-11.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0906 - out_caps_loss: 0.0805 - out_recon_loss: 0.0257 - out_caps_acc: 0.8925 - val_loss: 0.0860 - val_out_caps_loss: 0.0763 - val_out_recon_loss: 0.0247 - val_out_caps_acc: 0.8946\n",
      "Epoch 13/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0879 - out_caps_loss: 0.0781 - out_recon_loss: 0.0252 - out_caps_acc: 0.8961Epoch 00012: val_loss improved from 0.08597 to 0.08490, saving model to ./result/weights-12.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0879 - out_caps_loss: 0.0780 - out_recon_loss: 0.0252 - out_caps_acc: 0.8962 - val_loss: 0.0849 - val_out_caps_loss: 0.0753 - val_out_recon_loss: 0.0244 - val_out_caps_acc: 0.8968\n",
      "Epoch 14/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0864 - out_caps_loss: 0.0766 - out_recon_loss: 0.0248 - out_caps_acc: 0.8984Epoch 00013: val_loss improved from 0.08490 to 0.08407, saving model to ./result/weights-13.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0864 - out_caps_loss: 0.0766 - out_recon_loss: 0.0248 - out_caps_acc: 0.8984 - val_loss: 0.0841 - val_out_caps_loss: 0.0743 - val_out_recon_loss: 0.0248 - val_out_caps_acc: 0.9041\n",
      "Epoch 15/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0841 - out_caps_loss: 0.0744 - out_recon_loss: 0.0247 - out_caps_acc: 0.9013Epoch 00014: val_loss improved from 0.08407 to 0.08324, saving model to ./result/weights-14.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0841 - out_caps_loss: 0.0744 - out_recon_loss: 0.0247 - out_caps_acc: 0.9012 - val_loss: 0.0832 - val_out_caps_loss: 0.0738 - val_out_recon_loss: 0.0242 - val_out_caps_acc: 0.9020\n",
      "Epoch 16/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0814 - out_caps_loss: 0.0720 - out_recon_loss: 0.0241 - out_caps_acc: 0.9041Epoch 00015: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0814 - out_caps_loss: 0.0720 - out_recon_loss: 0.0241 - out_caps_acc: 0.9041 - val_loss: 0.0835 - val_out_caps_loss: 0.0741 - val_out_recon_loss: 0.0239 - val_out_caps_acc: 0.9017\n",
      "Epoch 17/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0810 - out_caps_loss: 0.0716 - out_recon_loss: 0.0239 - out_caps_acc: 0.9053Epoch 00016: val_loss improved from 0.08324 to 0.08087, saving model to ./result/weights-16.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0810 - out_caps_loss: 0.0716 - out_recon_loss: 0.0239 - out_caps_acc: 0.9053 - val_loss: 0.0809 - val_out_caps_loss: 0.0717 - val_out_recon_loss: 0.0233 - val_out_caps_acc: 0.9053\n",
      "Epoch 18/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0787 - out_caps_loss: 0.0695 - out_recon_loss: 0.0234 - out_caps_acc: 0.9063Epoch 00017: val_loss improved from 0.08087 to 0.07956, saving model to ./result/weights-17.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0787 - out_caps_loss: 0.0695 - out_recon_loss: 0.0234 - out_caps_acc: 0.9063 - val_loss: 0.0796 - val_out_caps_loss: 0.0706 - val_out_recon_loss: 0.0228 - val_out_caps_acc: 0.9041\n",
      "Epoch 19/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0775 - out_caps_loss: 0.0684 - out_recon_loss: 0.0233 - out_caps_acc: 0.9090Epoch 00018: val_loss improved from 0.07956 to 0.07663, saving model to ./result/weights-18.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0775 - out_caps_loss: 0.0684 - out_recon_loss: 0.0233 - out_caps_acc: 0.9091 - val_loss: 0.0766 - val_out_caps_loss: 0.0675 - val_out_recon_loss: 0.0234 - val_out_caps_acc: 0.9111\n",
      "Epoch 20/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0757 - out_caps_loss: 0.0667 - out_recon_loss: 0.0230 - out_caps_acc: 0.9106Epoch 00019: val_loss improved from 0.07663 to 0.07552, saving model to ./result/weights-19.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0757 - out_caps_loss: 0.0667 - out_recon_loss: 0.0230 - out_caps_acc: 0.9106 - val_loss: 0.0755 - val_out_caps_loss: 0.0667 - val_out_recon_loss: 0.0225 - val_out_caps_acc: 0.9131\n",
      "Epoch 21/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0753 - out_caps_loss: 0.0664 - out_recon_loss: 0.0228 - out_caps_acc: 0.9122Epoch 00020: val_loss improved from 0.07552 to 0.07425, saving model to ./result/weights-20.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0754 - out_caps_loss: 0.0665 - out_recon_loss: 0.0228 - out_caps_acc: 0.9121 - val_loss: 0.0743 - val_out_caps_loss: 0.0655 - val_out_recon_loss: 0.0223 - val_out_caps_acc: 0.9139\n",
      "Epoch 22/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0735 - out_caps_loss: 0.0647 - out_recon_loss: 0.0225 - out_caps_acc: 0.9141Epoch 00021: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0735 - out_caps_loss: 0.0647 - out_recon_loss: 0.0225 - out_caps_acc: 0.9141 - val_loss: 0.0749 - val_out_caps_loss: 0.0662 - val_out_recon_loss: 0.0220 - val_out_caps_acc: 0.9132\n",
      "Epoch 23/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0728 - out_caps_loss: 0.0640 - out_recon_loss: 0.0224 - out_caps_acc: 0.9140Epoch 00022: val_loss improved from 0.07425 to 0.07352, saving model to ./result/weights-22.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0728 - out_caps_loss: 0.0640 - out_recon_loss: 0.0224 - out_caps_acc: 0.9141 - val_loss: 0.0735 - val_out_caps_loss: 0.0648 - val_out_recon_loss: 0.0222 - val_out_caps_acc: 0.9132\n",
      "Epoch 24/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0723 - out_caps_loss: 0.0636 - out_recon_loss: 0.0221 - out_caps_acc: 0.9152Epoch 00023: val_loss improved from 0.07352 to 0.07341, saving model to ./result/weights-23.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0722 - out_caps_loss: 0.0636 - out_recon_loss: 0.0221 - out_caps_acc: 0.9152 - val_loss: 0.0734 - val_out_caps_loss: 0.0648 - val_out_recon_loss: 0.0220 - val_out_caps_acc: 0.9136\n",
      "Epoch 25/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0703 - out_caps_loss: 0.0617 - out_recon_loss: 0.0219 - out_caps_acc: 0.9175Epoch 00024: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0703 - out_caps_loss: 0.0617 - out_recon_loss: 0.0219 - out_caps_acc: 0.9176 - val_loss: 0.0753 - val_out_caps_loss: 0.0665 - val_out_recon_loss: 0.0225 - val_out_caps_acc: 0.9145\n",
      "Epoch 26/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0695 - out_caps_loss: 0.0610 - out_recon_loss: 0.0217 - out_caps_acc: 0.9184Epoch 00025: val_loss improved from 0.07341 to 0.07221, saving model to ./result/weights-25.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0695 - out_caps_loss: 0.0610 - out_recon_loss: 0.0217 - out_caps_acc: 0.9184 - val_loss: 0.0722 - val_out_caps_loss: 0.0638 - val_out_recon_loss: 0.0215 - val_out_caps_acc: 0.9164\n",
      "Epoch 27/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0680 - out_caps_loss: 0.0595 - out_recon_loss: 0.0215 - out_caps_acc: 0.9204Epoch 00026: val_loss improved from 0.07221 to 0.07123, saving model to ./result/weights-26.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0680 - out_caps_loss: 0.0596 - out_recon_loss: 0.0215 - out_caps_acc: 0.9204 - val_loss: 0.0712 - val_out_caps_loss: 0.0628 - val_out_recon_loss: 0.0216 - val_out_caps_acc: 0.9171\n",
      "Epoch 28/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0679 - out_caps_loss: 0.0595 - out_recon_loss: 0.0214 - out_caps_acc: 0.9208Epoch 00027: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 562s - loss: 0.0679 - out_caps_loss: 0.0595 - out_recon_loss: 0.0214 - out_caps_acc: 0.9207 - val_loss: 0.0731 - val_out_caps_loss: 0.0647 - val_out_recon_loss: 0.0214 - val_out_caps_acc: 0.9131\n",
      "Epoch 29/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0663 - out_caps_loss: 0.0580 - out_recon_loss: 0.0213 - out_caps_acc: 0.9234Epoch 00028: val_loss improved from 0.07123 to 0.06904, saving model to ./result/weights-28.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0663 - out_caps_loss: 0.0579 - out_recon_loss: 0.0213 - out_caps_acc: 0.9235 - val_loss: 0.0690 - val_out_caps_loss: 0.0608 - val_out_recon_loss: 0.0211 - val_out_caps_acc: 0.9180\n",
      "Epoch 30/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0660 - out_caps_loss: 0.0577 - out_recon_loss: 0.0211 - out_caps_acc: 0.9234Epoch 00029: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0660 - out_caps_loss: 0.0577 - out_recon_loss: 0.0211 - out_caps_acc: 0.9234 - val_loss: 0.0718 - val_out_caps_loss: 0.0637 - val_out_recon_loss: 0.0209 - val_out_caps_acc: 0.9157\n",
      "Epoch 31/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0651 - out_caps_loss: 0.0569 - out_recon_loss: 0.0209 - out_caps_acc: 0.9246Epoch 00030: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0650 - out_caps_loss: 0.0568 - out_recon_loss: 0.0209 - out_caps_acc: 0.9247 - val_loss: 0.0712 - val_out_caps_loss: 0.0630 - val_out_recon_loss: 0.0210 - val_out_caps_acc: 0.9149\n",
      "Epoch 32/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0644 - out_caps_loss: 0.0563 - out_recon_loss: 0.0208 - out_caps_acc: 0.9245Epoch 00031: val_loss improved from 0.06904 to 0.06822, saving model to ./result/weights-31.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0644 - out_caps_loss: 0.0563 - out_recon_loss: 0.0208 - out_caps_acc: 0.9245 - val_loss: 0.0682 - val_out_caps_loss: 0.0601 - val_out_recon_loss: 0.0207 - val_out_caps_acc: 0.9190\n",
      "Epoch 33/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0632 - out_caps_loss: 0.0550 - out_recon_loss: 0.0207 - out_caps_acc: 0.9276Epoch 00032: val_loss improved from 0.06822 to 0.06817, saving model to ./result/weights-32.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0632 - out_caps_loss: 0.0551 - out_recon_loss: 0.0207 - out_caps_acc: 0.9275 - val_loss: 0.0682 - val_out_caps_loss: 0.0601 - val_out_recon_loss: 0.0206 - val_out_caps_acc: 0.9204\n",
      "Epoch 34/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0629 - out_caps_loss: 0.0548 - out_recon_loss: 0.0206 - out_caps_acc: 0.9276Epoch 00033: val_loss improved from 0.06817 to 0.06677, saving model to ./result/weights-33.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0629 - out_caps_loss: 0.0548 - out_recon_loss: 0.0206 - out_caps_acc: 0.9275 - val_loss: 0.0668 - val_out_caps_loss: 0.0588 - val_out_recon_loss: 0.0204 - val_out_caps_acc: 0.9211\n",
      "Epoch 35/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0624 - out_caps_loss: 0.0543 - out_recon_loss: 0.0206 - out_caps_acc: 0.9276Epoch 00034: val_loss improved from 0.06677 to 0.06557, saving model to ./result/weights-34.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0624 - out_caps_loss: 0.0543 - out_recon_loss: 0.0206 - out_caps_acc: 0.9276 - val_loss: 0.0656 - val_out_caps_loss: 0.0575 - val_out_recon_loss: 0.0205 - val_out_caps_acc: 0.9223\n",
      "Epoch 36/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0612 - out_caps_loss: 0.0532 - out_recon_loss: 0.0205 - out_caps_acc: 0.9295Epoch 00035: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0612 - out_caps_loss: 0.0532 - out_recon_loss: 0.0205 - out_caps_acc: 0.9294 - val_loss: 0.0666 - val_out_caps_loss: 0.0586 - val_out_recon_loss: 0.0204 - val_out_caps_acc: 0.9219\n",
      "Epoch 37/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0612 - out_caps_loss: 0.0533 - out_recon_loss: 0.0203 - out_caps_acc: 0.9297Epoch 00036: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0612 - out_caps_loss: 0.0533 - out_recon_loss: 0.0203 - out_caps_acc: 0.9297 - val_loss: 0.0658 - val_out_caps_loss: 0.0578 - val_out_recon_loss: 0.0204 - val_out_caps_acc: 0.9221\n",
      "Epoch 38/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0606 - out_caps_loss: 0.0526 - out_recon_loss: 0.0202 - out_caps_acc: 0.9289Epoch 00037: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0606 - out_caps_loss: 0.0526 - out_recon_loss: 0.0202 - out_caps_acc: 0.9289 - val_loss: 0.0666 - val_out_caps_loss: 0.0587 - val_out_recon_loss: 0.0202 - val_out_caps_acc: 0.9198\n",
      "Epoch 39/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0600 - out_caps_loss: 0.0521 - out_recon_loss: 0.0202 - out_caps_acc: 0.9312Epoch 00038: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0600 - out_caps_loss: 0.0521 - out_recon_loss: 0.0202 - out_caps_acc: 0.9312 - val_loss: 0.0660 - val_out_caps_loss: 0.0580 - val_out_recon_loss: 0.0202 - val_out_caps_acc: 0.9220\n",
      "Epoch 40/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0596 - out_caps_loss: 0.0517 - out_recon_loss: 0.0201 - out_caps_acc: 0.9307Epoch 00039: val_loss improved from 0.06557 to 0.06505, saving model to ./result/weights-39.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0596 - out_caps_loss: 0.0517 - out_recon_loss: 0.0201 - out_caps_acc: 0.9307 - val_loss: 0.0650 - val_out_caps_loss: 0.0572 - val_out_recon_loss: 0.0200 - val_out_caps_acc: 0.9228\n",
      "Epoch 41/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0585 - out_caps_loss: 0.0506 - out_recon_loss: 0.0200 - out_caps_acc: 0.9329Epoch 00040: val_loss improved from 0.06505 to 0.06455, saving model to ./result/weights-40.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0585 - out_caps_loss: 0.0507 - out_recon_loss: 0.0200 - out_caps_acc: 0.9329 - val_loss: 0.0646 - val_out_caps_loss: 0.0567 - val_out_recon_loss: 0.0201 - val_out_caps_acc: 0.9230\n",
      "Epoch 42/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0587 - out_caps_loss: 0.0509 - out_recon_loss: 0.0199 - out_caps_acc: 0.9322Epoch 00041: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0587 - out_caps_loss: 0.0509 - out_recon_loss: 0.0199 - out_caps_acc: 0.9322 - val_loss: 0.0658 - val_out_caps_loss: 0.0580 - val_out_recon_loss: 0.0200 - val_out_caps_acc: 0.9220\n",
      "Epoch 43/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0575 - out_caps_loss: 0.0497 - out_recon_loss: 0.0199 - out_caps_acc: 0.9346Epoch 00042: val_loss improved from 0.06455 to 0.06425, saving model to ./result/weights-42.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0574 - out_caps_loss: 0.0496 - out_recon_loss: 0.0199 - out_caps_acc: 0.9346 - val_loss: 0.0643 - val_out_caps_loss: 0.0564 - val_out_recon_loss: 0.0199 - val_out_caps_acc: 0.9251\n",
      "Epoch 44/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0570 - out_caps_loss: 0.0493 - out_recon_loss: 0.0198 - out_caps_acc: 0.9346Epoch 00043: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0570 - out_caps_loss: 0.0493 - out_recon_loss: 0.0198 - out_caps_acc: 0.9346 - val_loss: 0.0652 - val_out_caps_loss: 0.0574 - val_out_recon_loss: 0.0199 - val_out_caps_acc: 0.9232\n",
      "Epoch 45/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0566 - out_caps_loss: 0.0488 - out_recon_loss: 0.0198 - out_caps_acc: 0.9349Epoch 00044: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0566 - out_caps_loss: 0.0488 - out_recon_loss: 0.0198 - out_caps_acc: 0.9349 - val_loss: 0.0647 - val_out_caps_loss: 0.0569 - val_out_recon_loss: 0.0198 - val_out_caps_acc: 0.9247\n",
      "Epoch 46/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0559 - out_caps_loss: 0.0482 - out_recon_loss: 0.0197 - out_caps_acc: 0.9355Epoch 00045: val_loss improved from 0.06425 to 0.06351, saving model to ./result/weights-45.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 562s - loss: 0.0559 - out_caps_loss: 0.0482 - out_recon_loss: 0.0197 - out_caps_acc: 0.9356 - val_loss: 0.0635 - val_out_caps_loss: 0.0558 - val_out_recon_loss: 0.0197 - val_out_caps_acc: 0.9242\n",
      "Epoch 47/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0557 - out_caps_loss: 0.0480 - out_recon_loss: 0.0196 - out_caps_acc: 0.9372Epoch 00046: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0557 - out_caps_loss: 0.0480 - out_recon_loss: 0.0196 - out_caps_acc: 0.9371 - val_loss: 0.0644 - val_out_caps_loss: 0.0567 - val_out_recon_loss: 0.0197 - val_out_caps_acc: 0.9221\n",
      "Epoch 48/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0557 - out_caps_loss: 0.0480 - out_recon_loss: 0.0196 - out_caps_acc: 0.9366Epoch 00047: val_loss improved from 0.06351 to 0.06332, saving model to ./result/weights-47.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0558 - out_caps_loss: 0.0481 - out_recon_loss: 0.0196 - out_caps_acc: 0.9365 - val_loss: 0.0633 - val_out_caps_loss: 0.0556 - val_out_recon_loss: 0.0196 - val_out_caps_acc: 0.9249\n",
      "Epoch 49/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0547 - out_caps_loss: 0.0470 - out_recon_loss: 0.0196 - out_caps_acc: 0.9374Epoch 00048: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0547 - out_caps_loss: 0.0470 - out_recon_loss: 0.0196 - out_caps_acc: 0.9373 - val_loss: 0.0636 - val_out_caps_loss: 0.0559 - val_out_recon_loss: 0.0196 - val_out_caps_acc: 0.9262\n",
      "Epoch 50/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0550 - out_caps_loss: 0.0474 - out_recon_loss: 0.0195 - out_caps_acc: 0.9367Epoch 00049: val_loss improved from 0.06332 to 0.06328, saving model to ./result/weights-49.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0550 - out_caps_loss: 0.0474 - out_recon_loss: 0.0195 - out_caps_acc: 0.9366 - val_loss: 0.0633 - val_out_caps_loss: 0.0556 - val_out_recon_loss: 0.0196 - val_out_caps_acc: 0.9240\n",
      "Epoch 51/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0547 - out_caps_loss: 0.0470 - out_recon_loss: 0.0195 - out_caps_acc: 0.9380Epoch 00050: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0547 - out_caps_loss: 0.0471 - out_recon_loss: 0.0195 - out_caps_acc: 0.9379 - val_loss: 0.0634 - val_out_caps_loss: 0.0557 - val_out_recon_loss: 0.0195 - val_out_caps_acc: 0.9250\n",
      "Epoch 52/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0541 - out_caps_loss: 0.0465 - out_recon_loss: 0.0195 - out_caps_acc: 0.9385Epoch 00051: val_loss improved from 0.06328 to 0.06289, saving model to ./result/weights-51.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0541 - out_caps_loss: 0.0465 - out_recon_loss: 0.0195 - out_caps_acc: 0.9385 - val_loss: 0.0629 - val_out_caps_loss: 0.0552 - val_out_recon_loss: 0.0196 - val_out_caps_acc: 0.9254\n",
      "Epoch 53/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0541 - out_caps_loss: 0.0465 - out_recon_loss: 0.0194 - out_caps_acc: 0.9385Epoch 00052: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0541 - out_caps_loss: 0.0465 - out_recon_loss: 0.0194 - out_caps_acc: 0.9385 - val_loss: 0.0634 - val_out_caps_loss: 0.0557 - val_out_recon_loss: 0.0196 - val_out_caps_acc: 0.9260\n",
      "Epoch 54/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0535 - out_caps_loss: 0.0459 - out_recon_loss: 0.0194 - out_caps_acc: 0.9398Epoch 00053: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0535 - out_caps_loss: 0.0459 - out_recon_loss: 0.0194 - out_caps_acc: 0.9399 - val_loss: 0.0631 - val_out_caps_loss: 0.0555 - val_out_recon_loss: 0.0194 - val_out_caps_acc: 0.9257\n",
      "Epoch 55/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0533 - out_caps_loss: 0.0457 - out_recon_loss: 0.0194 - out_caps_acc: 0.9401Epoch 00054: val_loss improved from 0.06289 to 0.06165, saving model to ./result/weights-54.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0533 - out_caps_loss: 0.0457 - out_recon_loss: 0.0194 - out_caps_acc: 0.9401 - val_loss: 0.0617 - val_out_caps_loss: 0.0540 - val_out_recon_loss: 0.0195 - val_out_caps_acc: 0.9277\n",
      "Epoch 56/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0527 - out_caps_loss: 0.0451 - out_recon_loss: 0.0193 - out_caps_acc: 0.9404Epoch 00055: val_loss improved from 0.06165 to 0.06154, saving model to ./result/weights-55.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0527 - out_caps_loss: 0.0451 - out_recon_loss: 0.0193 - out_caps_acc: 0.9404 - val_loss: 0.0615 - val_out_caps_loss: 0.0540 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9245\n",
      "Epoch 57/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0529 - out_caps_loss: 0.0453 - out_recon_loss: 0.0193 - out_caps_acc: 0.9394Epoch 00056: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0529 - out_caps_loss: 0.0453 - out_recon_loss: 0.0193 - out_caps_acc: 0.9394 - val_loss: 0.0625 - val_out_caps_loss: 0.0548 - val_out_recon_loss: 0.0195 - val_out_caps_acc: 0.9266\n",
      "Epoch 58/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0526 - out_caps_loss: 0.0450 - out_recon_loss: 0.0193 - out_caps_acc: 0.9410Epoch 00057: val_loss improved from 0.06154 to 0.06151, saving model to ./result/weights-57.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0526 - out_caps_loss: 0.0450 - out_recon_loss: 0.0193 - out_caps_acc: 0.9410 - val_loss: 0.0615 - val_out_caps_loss: 0.0539 - val_out_recon_loss: 0.0194 - val_out_caps_acc: 0.9276\n",
      "Epoch 59/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0518 - out_caps_loss: 0.0442 - out_recon_loss: 0.0193 - out_caps_acc: 0.9417Epoch 00058: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0518 - out_caps_loss: 0.0442 - out_recon_loss: 0.0193 - out_caps_acc: 0.9417 - val_loss: 0.0619 - val_out_caps_loss: 0.0543 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9267\n",
      "Epoch 60/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0517 - out_caps_loss: 0.0442 - out_recon_loss: 0.0192 - out_caps_acc: 0.9415Epoch 00059: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0517 - out_caps_loss: 0.0442 - out_recon_loss: 0.0192 - out_caps_acc: 0.9416 - val_loss: 0.0626 - val_out_caps_loss: 0.0550 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9240\n",
      "Epoch 61/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0516 - out_caps_loss: 0.0440 - out_recon_loss: 0.0192 - out_caps_acc: 0.9426Epoch 00060: val_loss improved from 0.06151 to 0.06125, saving model to ./result/weights-60.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0516 - out_caps_loss: 0.0440 - out_recon_loss: 0.0192 - out_caps_acc: 0.9426 - val_loss: 0.0612 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9277\n",
      "Epoch 62/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0520 - out_caps_loss: 0.0445 - out_recon_loss: 0.0192 - out_caps_acc: 0.9418Epoch 00061: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0520 - out_caps_loss: 0.0444 - out_recon_loss: 0.0192 - out_caps_acc: 0.9419 - val_loss: 0.0614 - val_out_caps_loss: 0.0538 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9271\n",
      "Epoch 63/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0515 - out_caps_loss: 0.0439 - out_recon_loss: 0.0192 - out_caps_acc: 0.9412Epoch 00062: val_loss improved from 0.06125 to 0.06098, saving model to ./result/weights-62.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0514 - out_caps_loss: 0.0439 - out_recon_loss: 0.0192 - out_caps_acc: 0.9412 - val_loss: 0.0610 - val_out_caps_loss: 0.0534 - val_out_recon_loss: 0.0192 - val_out_caps_acc: 0.9278\n",
      "Epoch 64/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0511 - out_caps_loss: 0.0436 - out_recon_loss: 0.0191 - out_caps_acc: 0.9422Epoch 00063: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 562s - loss: 0.0510 - out_caps_loss: 0.0436 - out_recon_loss: 0.0191 - out_caps_acc: 0.9422 - val_loss: 0.0612 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9285\n",
      "Epoch 65/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0502 - out_caps_loss: 0.0427 - out_recon_loss: 0.0191 - out_caps_acc: 0.9436Epoch 00064: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0502 - out_caps_loss: 0.0427 - out_recon_loss: 0.0191 - out_caps_acc: 0.9436 - val_loss: 0.0614 - val_out_caps_loss: 0.0538 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9270\n",
      "Epoch 66/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0509 - out_caps_loss: 0.0434 - out_recon_loss: 0.0191 - out_caps_acc: 0.9433Epoch 00065: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0508 - out_caps_loss: 0.0434 - out_recon_loss: 0.0191 - out_caps_acc: 0.9434 - val_loss: 0.0617 - val_out_caps_loss: 0.0542 - val_out_recon_loss: 0.0193 - val_out_caps_acc: 0.9279\n",
      "Epoch 67/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0508 - out_caps_loss: 0.0433 - out_recon_loss: 0.0191 - out_caps_acc: 0.9436Epoch 00066: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0508 - out_caps_loss: 0.0433 - out_recon_loss: 0.0191 - out_caps_acc: 0.9436 - val_loss: 0.0612 - val_out_caps_loss: 0.0536 - val_out_recon_loss: 0.0192 - val_out_caps_acc: 0.9279\n",
      "Epoch 68/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0501 - out_caps_loss: 0.0427 - out_recon_loss: 0.0190 - out_caps_acc: 0.9436Epoch 00067: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0501 - out_caps_loss: 0.0426 - out_recon_loss: 0.0190 - out_caps_acc: 0.9436 - val_loss: 0.0617 - val_out_caps_loss: 0.0541 - val_out_recon_loss: 0.0192 - val_out_caps_acc: 0.9254\n",
      "Epoch 69/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0506 - out_caps_loss: 0.0432 - out_recon_loss: 0.0190 - out_caps_acc: 0.9426Epoch 00068: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0506 - out_caps_loss: 0.0431 - out_recon_loss: 0.0190 - out_caps_acc: 0.9426 - val_loss: 0.0612 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9262\n",
      "Epoch 70/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0495 - out_caps_loss: 0.0421 - out_recon_loss: 0.0190 - out_caps_acc: 0.9447Epoch 00069: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0495 - out_caps_loss: 0.0421 - out_recon_loss: 0.0190 - out_caps_acc: 0.9447 - val_loss: 0.0612 - val_out_caps_loss: 0.0537 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9263\n",
      "Epoch 71/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0502 - out_caps_loss: 0.0427 - out_recon_loss: 0.0190 - out_caps_acc: 0.9437Epoch 00070: val_loss improved from 0.06098 to 0.06087, saving model to ./result/weights-70.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0502 - out_caps_loss: 0.0428 - out_recon_loss: 0.0190 - out_caps_acc: 0.9437 - val_loss: 0.0609 - val_out_caps_loss: 0.0534 - val_out_recon_loss: 0.0192 - val_out_caps_acc: 0.9274\n",
      "Epoch 72/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0501 - out_caps_loss: 0.0427 - out_recon_loss: 0.0190 - out_caps_acc: 0.9441Epoch 00071: val_loss improved from 0.06087 to 0.06055, saving model to ./result/weights-71.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0501 - out_caps_loss: 0.0426 - out_recon_loss: 0.0190 - out_caps_acc: 0.9441 - val_loss: 0.0605 - val_out_caps_loss: 0.0530 - val_out_recon_loss: 0.0192 - val_out_caps_acc: 0.9290\n",
      "Epoch 73/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0496 - out_caps_loss: 0.0421 - out_recon_loss: 0.0190 - out_caps_acc: 0.9446Epoch 00072: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0496 - out_caps_loss: 0.0421 - out_recon_loss: 0.0190 - out_caps_acc: 0.9446 - val_loss: 0.0610 - val_out_caps_loss: 0.0534 - val_out_recon_loss: 0.0192 - val_out_caps_acc: 0.9283\n",
      "Epoch 74/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0493 - out_caps_loss: 0.0418 - out_recon_loss: 0.0190 - out_caps_acc: 0.9456Epoch 00073: val_loss did not improve\n",
      "468/468 [==============================] - 562s - loss: 0.0493 - out_caps_loss: 0.0419 - out_recon_loss: 0.0190 - out_caps_acc: 0.9456 - val_loss: 0.0606 - val_out_caps_loss: 0.0531 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9277\n",
      "Epoch 75/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0499 - out_caps_loss: 0.0425 - out_recon_loss: 0.0190 - out_caps_acc: 0.9437Epoch 00074: val_loss improved from 0.06055 to 0.06048, saving model to ./result/weights-74.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0499 - out_caps_loss: 0.0425 - out_recon_loss: 0.0190 - out_caps_acc: 0.9437 - val_loss: 0.0605 - val_out_caps_loss: 0.0530 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9282\n",
      "Epoch 76/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0491 - out_caps_loss: 0.0417 - out_recon_loss: 0.0190 - out_caps_acc: 0.9460Epoch 00075: val_loss improved from 0.06048 to 0.06043, saving model to ./result/weights-75.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0492 - out_caps_loss: 0.0417 - out_recon_loss: 0.0190 - out_caps_acc: 0.9460 - val_loss: 0.0604 - val_out_caps_loss: 0.0529 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9291\n",
      "Epoch 77/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0498 - out_caps_loss: 0.0423 - out_recon_loss: 0.0190 - out_caps_acc: 0.9438Epoch 00076: val_loss improved from 0.06043 to 0.06034, saving model to ./result/weights-76.h5\n",
      "468/468 [==============================] - 562s - loss: 0.0498 - out_caps_loss: 0.0423 - out_recon_loss: 0.0190 - out_caps_acc: 0.9438 - val_loss: 0.0603 - val_out_caps_loss: 0.0529 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9293\n",
      "Epoch 78/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0493 - out_caps_loss: 0.0419 - out_recon_loss: 0.0190 - out_caps_acc: 0.9453Epoch 00077: val_loss did not improve\n",
      "468/468 [==============================] - 577s - loss: 0.0493 - out_caps_loss: 0.0419 - out_recon_loss: 0.0190 - out_caps_acc: 0.9454 - val_loss: 0.0606 - val_out_caps_loss: 0.0531 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9285\n",
      "Epoch 79/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0491 - out_caps_loss: 0.0417 - out_recon_loss: 0.0189 - out_caps_acc: 0.9465Epoch 00078: val_loss did not improve\n",
      "468/468 [==============================] - 594s - loss: 0.0491 - out_caps_loss: 0.0417 - out_recon_loss: 0.0189 - out_caps_acc: 0.9465 - val_loss: 0.0605 - val_out_caps_loss: 0.0530 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9295\n",
      "Epoch 80/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0492 - out_caps_loss: 0.0417 - out_recon_loss: 0.0189 - out_caps_acc: 0.9448Epoch 00079: val_loss improved from 0.06034 to 0.06025, saving model to ./result/weights-79.h5\n",
      "468/468 [==============================] - 594s - loss: 0.0492 - out_caps_loss: 0.0417 - out_recon_loss: 0.0189 - out_caps_acc: 0.9449 - val_loss: 0.0603 - val_out_caps_loss: 0.0528 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9297\n",
      "Epoch 81/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0494 - out_caps_loss: 0.0419 - out_recon_loss: 0.0189 - out_caps_acc: 0.9454Epoch 00080: val_loss did not improve\n",
      "468/468 [==============================] - 591s - loss: 0.0493 - out_caps_loss: 0.0419 - out_recon_loss: 0.0189 - out_caps_acc: 0.9455 - val_loss: 0.0604 - val_out_caps_loss: 0.0529 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 82/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0493 - out_caps_loss: 0.0418 - out_recon_loss: 0.0189 - out_caps_acc: 0.9444Epoch 00081: val_loss improved from 0.06025 to 0.06001, saving model to ./result/weights-81.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 591s - loss: 0.0493 - out_caps_loss: 0.0418 - out_recon_loss: 0.0189 - out_caps_acc: 0.9444 - val_loss: 0.0600 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9284\n",
      "Epoch 83/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0488 - out_caps_loss: 0.0414 - out_recon_loss: 0.0189 - out_caps_acc: 0.9455Epoch 00082: val_loss did not improve\n",
      "468/468 [==============================] - 591s - loss: 0.0488 - out_caps_loss: 0.0414 - out_recon_loss: 0.0189 - out_caps_acc: 0.9454 - val_loss: 0.0604 - val_out_caps_loss: 0.0529 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9270\n",
      "Epoch 84/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0486 - out_caps_loss: 0.0412 - out_recon_loss: 0.0189 - out_caps_acc: 0.9461Epoch 00083: val_loss did not improve\n",
      "468/468 [==============================] - 591s - loss: 0.0486 - out_caps_loss: 0.0412 - out_recon_loss: 0.0189 - out_caps_acc: 0.9461 - val_loss: 0.0603 - val_out_caps_loss: 0.0528 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9282\n",
      "Epoch 85/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0485 - out_caps_loss: 0.0411 - out_recon_loss: 0.0189 - out_caps_acc: 0.9467Epoch 00084: val_loss did not improve\n",
      "468/468 [==============================] - 591s - loss: 0.0485 - out_caps_loss: 0.0411 - out_recon_loss: 0.0189 - out_caps_acc: 0.9468 - val_loss: 0.0602 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9285\n",
      "Epoch 86/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0492 - out_caps_loss: 0.0418 - out_recon_loss: 0.0189 - out_caps_acc: 0.9448Epoch 00085: val_loss did not improve\n",
      "468/468 [==============================] - 591s - loss: 0.0492 - out_caps_loss: 0.0418 - out_recon_loss: 0.0189 - out_caps_acc: 0.9449 - val_loss: 0.0603 - val_out_caps_loss: 0.0528 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9289\n",
      "Epoch 87/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9465Epoch 00086: val_loss did not improve\n",
      "468/468 [==============================] - 591s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9465 - val_loss: 0.0600 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0191 - val_out_caps_acc: 0.9289\n",
      "Epoch 88/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0484 - out_caps_loss: 0.0410 - out_recon_loss: 0.0188 - out_caps_acc: 0.9458Epoch 00087: val_loss did not improve\n",
      "468/468 [==============================] - 593s - loss: 0.0484 - out_caps_loss: 0.0410 - out_recon_loss: 0.0188 - out_caps_acc: 0.9458 - val_loss: 0.0603 - val_out_caps_loss: 0.0529 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9286\n",
      "Epoch 89/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0487 - out_caps_loss: 0.0412 - out_recon_loss: 0.0190 - out_caps_acc: 0.9461Epoch 00088: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0486 - out_caps_loss: 0.0412 - out_recon_loss: 0.0190 - out_caps_acc: 0.9462 - val_loss: 0.0602 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 90/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9459Epoch 00089: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9459 - val_loss: 0.0601 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9286\n",
      "Epoch 91/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0487 - out_caps_loss: 0.0413 - out_recon_loss: 0.0189 - out_caps_acc: 0.9460Epoch 00090: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0487 - out_caps_loss: 0.0413 - out_recon_loss: 0.0189 - out_caps_acc: 0.9460 - val_loss: 0.0603 - val_out_caps_loss: 0.0528 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 92/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0406 - out_recon_loss: 0.0189 - out_caps_acc: 0.9472Epoch 00091: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0189 - out_caps_acc: 0.9472 - val_loss: 0.0601 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 93/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481Epoch 00092: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0479 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0600 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 94/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0189 - out_caps_acc: 0.9470Epoch 00093: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0189 - out_caps_acc: 0.9470 - val_loss: 0.0600 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 95/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0479 - out_caps_loss: 0.0405 - out_recon_loss: 0.0189 - out_caps_acc: 0.9471Epoch 00094: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0479 - out_caps_loss: 0.0405 - out_recon_loss: 0.0189 - out_caps_acc: 0.9471 - val_loss: 0.0602 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 96/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0482 - out_caps_loss: 0.0409 - out_recon_loss: 0.0188 - out_caps_acc: 0.9466Epoch 00095: val_loss did not improve\n",
      "468/468 [==============================] - 606s - loss: 0.0482 - out_caps_loss: 0.0409 - out_recon_loss: 0.0188 - out_caps_acc: 0.9467 - val_loss: 0.0602 - val_out_caps_loss: 0.0527 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9285\n",
      "Epoch 97/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0487 - out_caps_loss: 0.0413 - out_recon_loss: 0.0189 - out_caps_acc: 0.9452Epoch 00096: val_loss improved from 0.06001 to 0.05999, saving model to ./result/weights-96.h5\n",
      "468/468 [==============================] - 607s - loss: 0.0487 - out_caps_loss: 0.0413 - out_recon_loss: 0.0189 - out_caps_acc: 0.9452 - val_loss: 0.0600 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9285\n",
      "Epoch 98/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481Epoch 00097: val_loss did not improve\n",
      "468/468 [==============================] - 607s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0600 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9279\n",
      "Epoch 99/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0189 - out_caps_acc: 0.9473Epoch 00098: val_loss improved from 0.05999 to 0.05987, saving model to ./result/weights-98.h5\n",
      "468/468 [==============================] - 607s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 100/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0189 - out_caps_acc: 0.9467Epoch 00099: val_loss improved from 0.05987 to 0.05987, saving model to ./result/weights-99.h5\n",
      "468/468 [==============================] - 607s - loss: 0.0479 - out_caps_loss: 0.0404 - out_recon_loss: 0.0189 - out_caps_acc: 0.9467 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9287\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "467/468 [============================>.] - ETA: 1s - loss: 0.0477 - out_caps_loss: 0.0404 - out_recon_loss: 0.0188 - out_caps_acc: 0.9475Epoch 00100: val_loss did not improve\n",
      "468/468 [==============================] - 607s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0603 - val_out_caps_loss: 0.0528 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 102/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0189 - out_caps_acc: 0.9475Epoch 00101: val_loss did not improve\n",
      "468/468 [==============================] - 605s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0189 - out_caps_acc: 0.9476 - val_loss: 0.0600 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9281\n",
      "Epoch 103/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0482 - out_caps_loss: 0.0408 - out_recon_loss: 0.0189 - out_caps_acc: 0.9453Epoch 00102: val_loss did not improve\n",
      "468/468 [==============================] - 599s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0189 - out_caps_acc: 0.9453 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 104/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0479 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9468Epoch 00103: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9468 - val_loss: 0.0600 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9281\n",
      "Epoch 105/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9472Epoch 00104: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9472 - val_loss: 0.0601 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9299\n",
      "Epoch 106/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0189 - out_caps_acc: 0.9480Epoch 00105: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0189 - out_caps_acc: 0.9480 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 107/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9492Epoch 00106: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9492 - val_loss: 0.0601 - val_out_caps_loss: 0.0526 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 108/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0480 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474Epoch 00107: val_loss improved from 0.05987 to 0.05987, saving model to ./result/weights-107.h5\n",
      "468/468 [==============================] - 595s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9287\n",
      "Epoch 109/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0189 - out_caps_acc: 0.9473Epoch 00108: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0189 - out_caps_acc: 0.9473 - val_loss: 0.0599 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 110/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9478Epoch 00109: val_loss improved from 0.05987 to 0.05982, saving model to ./result/weights-109.h5\n",
      "468/468 [==============================] - 597s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9285\n",
      "Epoch 111/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485Epoch 00110: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0471 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 112/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481Epoch 00111: val_loss improved from 0.05982 to 0.05971, saving model to ./result/weights-111.h5\n",
      "468/468 [==============================] - 597s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9285\n",
      "Epoch 113/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0189 - out_caps_acc: 0.9469Epoch 00112: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0189 - out_caps_acc: 0.9468 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9285\n",
      "Epoch 114/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9478Epoch 00113: val_loss did not improve\n",
      "468/468 [==============================] - 599s - loss: 0.0476 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9478 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 115/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474Epoch 00114: val_loss did not improve\n",
      "468/468 [==============================] - 599s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 116/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485Epoch 00115: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9285\n",
      "Epoch 117/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9477Epoch 00116: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0477 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 118/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9477Epoch 00117: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9477 - val_loss: 0.0599 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 119/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0469 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483Epoch 00118: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 597s - loss: 0.0469 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483 - val_loss: 0.0599 - val_out_caps_loss: 0.0525 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9284\n",
      "Epoch 120/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0408 - out_recon_loss: 0.0188 - out_caps_acc: 0.9472Epoch 00119: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0481 - out_caps_loss: 0.0408 - out_recon_loss: 0.0188 - out_caps_acc: 0.9472 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 121/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480Epoch 00120: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9294\n",
      "Epoch 122/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9478Epoch 00121: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9477 - val_loss: 0.0599 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9298\n",
      "Epoch 123/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482Epoch 00122: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0189 - out_caps_acc: 0.9483 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 124/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0478 - out_caps_loss: 0.0405 - out_recon_loss: 0.0187 - out_caps_acc: 0.9464Epoch 00123: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0187 - out_caps_acc: 0.9465 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 125/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486Epoch 00124: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0470 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 126/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484Epoch 00125: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 127/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9471Epoch 00126: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0474 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9470 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 128/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0477 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9470Epoch 00127: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0477 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9470 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 129/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9498Epoch 00128: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9497 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 130/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0469 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483Epoch 00129: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 131/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9477Epoch 00130: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9477 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 132/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0402 - out_recon_loss: 0.0187 - out_caps_acc: 0.9474Epoch 00131: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0187 - out_caps_acc: 0.9474 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 133/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482Epoch 00132: val_loss improved from 0.05971 to 0.05970, saving model to ./result/weights-132.h5\n",
      "468/468 [==============================] - 601s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9293\n",
      "Epoch 134/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0188 - out_caps_acc: 0.9470Epoch 00133: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0188 - out_caps_acc: 0.9470 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 135/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484Epoch 00134: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9287\n",
      "Epoch 136/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0468 - out_caps_loss: 0.0394 - out_recon_loss: 0.0188 - out_caps_acc: 0.9493Epoch 00135: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0468 - out_caps_loss: 0.0394 - out_recon_loss: 0.0188 - out_caps_acc: 0.9493 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 137/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482Epoch 00136: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 138/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481Epoch 00137: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 601s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 139/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00138: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0473 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480 - val_loss: 0.0598 - val_out_caps_loss: 0.0524 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 140/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00139: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 141/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476Epoch 00140: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9286\n",
      "Epoch 142/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0479 - out_caps_loss: 0.0405 - out_recon_loss: 0.0188 - out_caps_acc: 0.9467Epoch 00141: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9466 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 143/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9490Epoch 00142: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9491 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 144/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474Epoch 00143: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 145/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484Epoch 00144: val_loss did not improve\n",
      "468/468 [==============================] - 601s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 146/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486Epoch 00145: val_loss did not improve\n",
      "468/468 [==============================] - 600s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 147/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482Epoch 00146: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0189 - out_caps_acc: 0.9482 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 148/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483Epoch 00147: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 149/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0465 - out_caps_loss: 0.0391 - out_recon_loss: 0.0187 - out_caps_acc: 0.9487Epoch 00148: val_loss did not improve\n",
      "468/468 [==============================] - 600s - loss: 0.0464 - out_caps_loss: 0.0391 - out_recon_loss: 0.0187 - out_caps_acc: 0.9487 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 150/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9490Epoch 00149: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9490 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 151/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0469 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484Epoch 00150: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0469 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 152/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0467 - out_caps_loss: 0.0394 - out_recon_loss: 0.0187 - out_caps_acc: 0.9494Epoch 00151: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0468 - out_caps_loss: 0.0394 - out_recon_loss: 0.0187 - out_caps_acc: 0.9493 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 153/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480Epoch 00152: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 154/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00153: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0473 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 155/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0469 - out_caps_loss: 0.0396 - out_recon_loss: 0.0187 - out_caps_acc: 0.9486Epoch 00154: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0469 - out_caps_loss: 0.0396 - out_recon_loss: 0.0187 - out_caps_acc: 0.9485 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 156/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0189 - out_caps_acc: 0.9478Epoch 00155: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0189 - out_caps_acc: 0.9478 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 157/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485Epoch 00156: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 596s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 158/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00157: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 159/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9464Epoch 00158: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0481 - out_caps_loss: 0.0407 - out_recon_loss: 0.0188 - out_caps_acc: 0.9463 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 160/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0465 - out_caps_loss: 0.0392 - out_recon_loss: 0.0187 - out_caps_acc: 0.9493Epoch 00159: val_loss did not improve\n",
      "468/468 [==============================] - 594s - loss: 0.0466 - out_caps_loss: 0.0392 - out_recon_loss: 0.0187 - out_caps_acc: 0.9493 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 161/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9488Epoch 00160: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9488 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 162/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481Epoch 00161: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 163/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00162: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 164/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00163: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 165/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483Epoch 00164: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0470 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 166/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0479 - out_caps_loss: 0.0405 - out_recon_loss: 0.0188 - out_caps_acc: 0.9472Epoch 00165: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0479 - out_caps_loss: 0.0405 - out_recon_loss: 0.0188 - out_caps_acc: 0.9472 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 167/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0467 - out_caps_loss: 0.0393 - out_recon_loss: 0.0188 - out_caps_acc: 0.9494Epoch 00166: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0467 - out_caps_loss: 0.0393 - out_recon_loss: 0.0188 - out_caps_acc: 0.9495 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 168/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00167: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0478 - out_caps_loss: 0.0404 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 169/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0471 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9489Epoch 00168: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0471 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9489 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 170/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0469 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9488Epoch 00169: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0469 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9487 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 171/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0467 - out_caps_loss: 0.0393 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486Epoch 00170: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0467 - out_caps_loss: 0.0393 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 172/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9473Epoch 00171: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0480 - out_caps_loss: 0.0406 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 173/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9487Epoch 00172: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9488 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 174/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474Epoch 00173: val_loss did not improve\n",
      "468/468 [==============================] - 594s - loss: 0.0476 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 175/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481Epoch 00174: val_loss did not improve\n",
      "468/468 [==============================] - 594s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 176/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0466 - out_caps_loss: 0.0392 - out_recon_loss: 0.0188 - out_caps_acc: 0.9491Epoch 00175: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 596s - loss: 0.0465 - out_caps_loss: 0.0392 - out_recon_loss: 0.0188 - out_caps_acc: 0.9491 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 177/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476Epoch 00176: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0473 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 178/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00177: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0598 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9287\n",
      "Epoch 179/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9485Epoch 00178: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 180/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0467 - out_caps_loss: 0.0394 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00179: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0468 - out_caps_loss: 0.0394 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 181/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9473Epoch 00180: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9473 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 182/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9487Epoch 00181: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 183/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9478Epoch 00182: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0474 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 184/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00183: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 185/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0189 - out_caps_acc: 0.9477Epoch 00184: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0189 - out_caps_acc: 0.9476 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 186/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0468 - out_caps_loss: 0.0394 - out_recon_loss: 0.0188 - out_caps_acc: 0.9489Epoch 00185: val_loss did not improve\n",
      "468/468 [==============================] - 595s - loss: 0.0468 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9489 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 187/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9475Epoch 00186: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9474 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 188/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0476 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00187: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0476 - out_caps_loss: 0.0403 - out_recon_loss: 0.0188 - out_caps_acc: 0.9483 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 189/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00188: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9480 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 190/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484Epoch 00189: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0475 - out_caps_loss: 0.0402 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 191/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9488Epoch 00190: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0470 - out_caps_loss: 0.0396 - out_recon_loss: 0.0188 - out_caps_acc: 0.9489 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9287\n",
      "Epoch 192/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9482Epoch 00191: val_loss did not improve\n",
      "468/468 [==============================] - 597s - loss: 0.0473 - out_caps_loss: 0.0400 - out_recon_loss: 0.0188 - out_caps_acc: 0.9481 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 193/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0464 - out_caps_loss: 0.0390 - out_recon_loss: 0.0188 - out_caps_acc: 0.9495Epoch 00192: val_loss did not improve\n",
      "468/468 [==============================] - 596s - loss: 0.0464 - out_caps_loss: 0.0390 - out_recon_loss: 0.0188 - out_caps_acc: 0.9495 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9288\n",
      "Epoch 194/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0477 - out_caps_loss: 0.0403 - out_recon_loss: 0.0189 - out_caps_acc: 0.9474Epoch 00193: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0477 - out_caps_loss: 0.0403 - out_recon_loss: 0.0189 - out_caps_acc: 0.9474 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 195/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484Epoch 00194: val_loss did not improve\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - 595s - loss: 0.0473 - out_caps_loss: 0.0399 - out_recon_loss: 0.0188 - out_caps_acc: 0.9484 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 196/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0471 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486Epoch 00195: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0470 - out_caps_loss: 0.0397 - out_recon_loss: 0.0188 - out_caps_acc: 0.9486 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9292\n",
      "Epoch 197/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0475 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476Epoch 00196: val_loss did not improve\n",
      "468/468 [==============================] - 599s - loss: 0.0474 - out_caps_loss: 0.0401 - out_recon_loss: 0.0188 - out_caps_acc: 0.9476 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Epoch 198/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9488Epoch 00197: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0472 - out_caps_loss: 0.0398 - out_recon_loss: 0.0188 - out_caps_acc: 0.9489 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9290\n",
      "Epoch 199/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0469 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479Epoch 00198: val_loss did not improve\n",
      "468/468 [==============================] - 599s - loss: 0.0469 - out_caps_loss: 0.0395 - out_recon_loss: 0.0188 - out_caps_acc: 0.9479 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9289\n",
      "Epoch 200/200\n",
      "467/468 [============================>.] - ETA: 1s - loss: 0.0486 - out_caps_loss: 0.0412 - out_recon_loss: 0.0188 - out_caps_acc: 0.9466Epoch 00199: val_loss did not improve\n",
      "468/468 [==============================] - 598s - loss: 0.0485 - out_caps_loss: 0.0411 - out_recon_loss: 0.0188 - out_caps_acc: 0.9467 - val_loss: 0.0597 - val_out_caps_loss: 0.0523 - val_out_recon_loss: 0.0190 - val_out_caps_acc: 0.9291\n",
      "Trained model saved to './result/trained_model.h5'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAARwAAAG0CAYAAADgu5hjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f13d3843240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, data):\n",
    "    \"\"\"\n",
    "    Training a CapsuleNet\n",
    "    :param model: the CapsuleNet model\n",
    "    :param data: a tuple containing training and testing data, like `((x_train, y_train), (x_test, y_test))`\n",
    "    :param args: arguments\n",
    "    :return: The trained model\n",
    "    \"\"\"\n",
    "    # unpacking the data\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "\n",
    "    # callbacks\n",
    "    log = callbacks.CSVLogger(save_dir + '/log.csv')\n",
    "    tb = callbacks.TensorBoard(log_dir=save_dir + '/tensorboard-logs',\n",
    "                               batch_size=batch_size, histogram_freq=debug)\n",
    "    checkpoint = callbacks.ModelCheckpoint(save_dir + '/weights-{epoch:02d}.h5',\n",
    "                                           save_best_only=True, save_weights_only=True, verbose=1)\n",
    "    lr_decay = callbacks.LearningRateScheduler(schedule=lambda epoch: lr * (0.95 ** epoch))\n",
    "\n",
    "    # compile the model\n",
    "    model.compile(optimizer=optimizers.Adam(lr=lr),\n",
    "                  loss=[margin_loss, 'mse'],\n",
    "                  loss_weights=[1., lam_recon],\n",
    "                  metrics={'out_caps': 'accuracy'})\n",
    "\n",
    "    \"\"\"\n",
    "    # Training without data augmentation:\n",
    "    model.fit([x_train, y_train], [y_train, x_train], batch_size=args.batch_size, epochs=args.epochs,\n",
    "              validation_data=[[x_test, y_test], [y_test, x_test]], callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    \"\"\"\n",
    "\n",
    "    # Begin: Training with data augmentation ---------------------------------------------------------------------#\n",
    "    def train_generator(x, y, batch_size, shift_fraction=0.):\n",
    "        train_datagen = ImageDataGenerator(width_shift_range=shift_fraction,\n",
    "                                           height_shift_range=shift_fraction,\n",
    "                                           horizontal_flip=True)  # shift up to 2 pixel for MNIST\n",
    "        generator = train_datagen.flow(x, y, batch_size=batch_size)\n",
    "        while 1:\n",
    "            x_batch, y_batch = generator.next()\n",
    "            yield ([x_batch, y_batch], [y_batch, x_batch])\n",
    "\n",
    "    # Training with data augmentation. If shift_fraction=0., also no augmentation.\n",
    "    model.fit_generator(generator=train_generator(x_train, y_train, batch_size, shift_fraction),\n",
    "                        steps_per_epoch=int(y_train.shape[0] / batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=[[x_test, y_test], [y_test, x_test]],\n",
    "                        callbacks=[log, tb, checkpoint, lr_decay])\n",
    "    # End: Training with data augmentation -----------------------------------------------------------------------#\n",
    "\n",
    "    model.save_weights(save_dir + '/trained_model.h5')\n",
    "    print('Trained model saved to \\'%s/trained_model.h5\\'' % save_dir)\n",
    "\n",
    "    from utils import plot_log\n",
    "    plot_log(save_dir + '/log.csv', show=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "# Define Test function\n",
    "\n",
    "def test(model, data):\n",
    "    x_test, y_test = data\n",
    "    y_pred, x_recon = model.predict([x_test, y_test], batch_size=100)\n",
    "    print('-'*50)\n",
    "    print('Test acc:', np.sum(np.argmax(y_pred, 1) == np.argmax(y_test, 1))/y_test.shape[0])\n",
    "    img = combine_images(np.concatenate([x_test[:50],x_recon[:50]]))\n",
    "    image = img * 255\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"real_and_recon.png\")\n",
    "    print()\n",
    "    print('Reconstructed images are saved to ./real_and_recon.png')\n",
    "    print('-'*50)\n",
    "    plt.imshow(plt.imread(\"real_and_recon.png\", ))\n",
    "    plt.show()\n",
    "    \n",
    "# Define load Fashion MNIST data Fuction\n",
    "def load_mnist():\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = fashion.load_data()\n",
    "\n",
    "    x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.\n",
    "    y_train = to_categorical(y_train.astype('float32'))\n",
    "    y_test = to_categorical(y_test.astype('float32'))\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "batch_size     = 128\n",
    "epochs         = 200\n",
    "lam_recon      = 0.392  # 784 * 0.0005, paper uses sum of SE, here uses MSE\n",
    "num_routing    = 3      # num_routing should > 0\n",
    "shift_fraction = 0.1\n",
    "debug          = 0      # debug>0 will save weights by TensorBoard\n",
    "save_dir       ='./result'\n",
    "is_training    = 1\n",
    "weights        = None\n",
    "lr             = 0.001\n",
    "\n",
    "    \n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# load data\n",
    "(x_train, y_train), (x_test, y_test) = load_mnist()\n",
    "\n",
    "# define model\n",
    "model = CapsNet(input_shape=[28, 28, 1],\n",
    "                n_class=len(np.unique(np.argmax(y_train, 1))),\n",
    "                num_routing=num_routing)\n",
    "    \n",
    "model.summary()\n",
    "\n",
    "    \n",
    "# train or test\n",
    "if weights is not None:  # init the model weights with provided one\n",
    "    model.load_weights(weights)\n",
    "if is_training:\n",
    "    train(model=model, data=((x_train, y_train), (x_test, y_test)))\n",
    "else:  # as long as weights are given, will run testing\n",
    "    if weights is None:\n",
    "        print('No weights are provided. Will test using random initialized weights.')\n",
    "    test(model=model, data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
