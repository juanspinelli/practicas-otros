{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Tensor Comprehensions\n",
    "\n",
    "```shell\n",
    "$ conda install -y -c pytorch -c tensorcomp tensor_comprehensions\n",
    "```\n",
    "Note: Won;t work on your mac, this is my Ubuntu server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensor_comprehensions as tc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "def matmul(float(M,N) A, float(N,K) B) -> (output) {\n",
    "  output(i, j) +=! A(i, kk) * B(kk, j)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(3, 4).cuda(), torch.randn(4, 5).cuda()\n",
    "out = matmul(mat1, mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.3949  0.8303 -1.5782  1.3349 -0.0063\n",
       "-2.8526 -0.1465  4.4961 -4.9543  2.0988\n",
       "-1.7459  0.9724  0.9590  1.3005  4.0354\n",
       "[torch.cuda.FloatTensor of size 3x5 (GPU 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch layers in Tensor Comprehensions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use of mapping option\n",
    "\n",
    "Default Mapping: We provide various default options that can be chosen to closely represent the kernel. The defaults provided are:\n",
    "\n",
    "- `pointwise, color=red`: if kernel resembles a pointwise operation\n",
    "- `mlp`: if kernel resembles an Linear layer operation\n",
    "- `conv`: if kernel resembles a convolution operation\n",
    "- `group_conv`: if kernel resembles a group convolution operation\n",
    "- `naive`: if none of the above, then chose naive default <-- This is why we get the warning\n",
    "<font color='red'>bar</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specifying mapping options\n",
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(100, 400).cuda(), torch.randn(400, 500).cuda()\n",
    "out2 = matmul(mat1, mat2, options=tc.Options(\"mlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-9.3951e+00 -1.8997e+01 -1.7598e+01  ...  -5.4261e+01 -1.5804e+01 -2.2281e+00\n",
       "-1.7843e+01  9.8855e+00  4.3533e+01  ...   9.5547e+00  1.7750e+01  6.3619e-01\n",
       "-1.5394e+01  1.0743e+01 -2.8723e+01  ...  -2.8901e+00 -4.9826e+00 -3.4955e+01\n",
       "                ...                   ⋱                   ...                \n",
       " 7.9932e+00 -2.8686e+01  8.3991e+00  ...  -2.2157e+00  3.9128e+01 -2.7361e+01\n",
       " 1.6213e+01  4.1226e+00  3.3095e+01  ...   3.0941e+01  2.5200e+00 -1.5295e+01\n",
       "-6.6816e+00 -3.2069e+01  3.0220e+01  ...  -6.3537e+00 -2.4188e-01  9.4838e+00\n",
       "[torch.cuda.FloatTensor of size 100x500 (GPU 0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n",
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n",
      "Variable containing:\n",
      " 1.1297  1.2930  0.2438  0.2914  1.3435\n",
      "-2.0003 -0.0755 -1.7224 -0.0140 -0.8154\n",
      "-1.9083  0.7932 -1.7260  0.6538 -2.2691\n",
      "[torch.cuda.FloatTensor of size 3x5 (GPU 0)]\n",
      "\n",
      "Variable containing:\n",
      "-28.1100 -11.6430  -7.6212  ...    3.3825  18.4664  -8.4693\n",
      " 16.3596  21.3456   2.9245  ...   26.5358  -6.3813   2.3102\n",
      " 31.8404 -10.3774  -5.6214  ...  -36.0098  -7.2178  21.7369\n",
      "           ...               ⋱              ...            \n",
      "  8.2418   3.6604  -8.9530  ...   14.3242   3.1555  -3.3104\n",
      " 10.2438 -28.3403  24.1943  ...   -5.6581   3.6120  22.8566\n",
      " -2.3919 -24.7140  -8.9158  ...  -18.7468  -0.5242   4.8057\n",
      "[torch.cuda.FloatTensor of size 100x500 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using reduction operators\n",
    "# providing different input sizes for the same comprehension\n",
    "\n",
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(3, 4).cuda(), torch.randn(4, 5).cuda()\n",
    "out = matmul(mat1, mat2)\n",
    "\n",
    "# different input sizes\n",
    "mat3, mat4 = torch.randn(100, 400).cuda(), torch.randn(400, 500).cuda()\n",
    "out2 = matmul(mat3, mat4)\n",
    "print(out)\n",
    "print(out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple TC definitions\n",
    "\n",
    "Let’s say you want to define all of your TCs in one string and later use that string for running different operations defined in the string. You an do so easily. You can define a <font color='blue'>lang</font> variable that holds the TC definition for all your operations. Every time you want to run a different operation, you can make a <font color='blue'>tc.define</font> call on the <font color='blue'>lang</font> variable, specify the <font color='blue'>name</font> corresponding to the operation definition and get the TC layer for it. Below is an example for how to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n",
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "lang = \"\"\"\n",
    "def matmul(float(M,N) A, float(N,K) B) -> (output) {\n",
    "  output(i, j) +=! A(i, kk) * B(kk, j)\n",
    "}\n",
    "def abs(float(M, N) A) -> (O1) {\n",
    "  O1(m, n) = fabs(A(m, n))\n",
    "}\n",
    "\"\"\"\n",
    "matmul = tc.define(lang, name=\"matmul\")\n",
    "mat1, mat2 = torch.randn(3, 4).cuda(), torch.randn(4, 5).cuda()\n",
    "out = matmul(mat1, mat2)\n",
    "\n",
    "abs = tc.define(lang, name=\"abs\")\n",
    "A = torch.randn(3, 4).cuda()\n",
    "out = abs(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1564  0.6053  2.4715  1.3225\n",
       " 1.4164  1.2355  0.4340  0.1880\n",
       " 2.4908  1.8639  0.1854  0.0301\n",
       "[torch.cuda.FloatTensor of size 3x4 (GPU 0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Writing layers with Scalars\n",
    "\n",
    "- **Option 1**: Pass a constants dictionary to the tc.define call as demo'ed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "def avgpool(float(B, C, H, W) input) -> (output) {{\n",
    "    output(b, c, h, w) += input(b, c, h * {sH} + kh, w * {sW} + kw) where kh in 0:{kH}, kw in 0:{kW}\n",
    "}}\n",
    "\"\"\"\n",
    "avgpool = tc.define(lang, name=\"avgpool\", constants={\"sH\":1, \"sW\":1, \"kH\":2, \"kW\":2})\n",
    "inp = torch.ones(32, 3, 10, 10).cuda()\n",
    "out4 = avgpool(inp, options=tc.Options(\"mlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "(0 ,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(0 ,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(0 ,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(1 ,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(1 ,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(1 ,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(2 ,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(2 ,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(2 ,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "...   \n",
       "     ⋮ \n",
       "\n",
       "(29,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(29,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(29,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(30,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(30,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(30,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ⋮ \n",
       "\n",
       "(31,0 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(31,1 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "\n",
       "(31,2 ,.,.) = \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "     ...       ⋱       ...    \n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "   4   4   4  ...    4   4   4\n",
       "[torch.cuda.FloatTensor of size 32x3x9x9 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2 : Format the string using python regex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "LANG=\"\"\"\n",
    "def avgpool(float(B, C, H, W) input) -> (output) {\n",
    "    output(b, c, h, w) += input(b, c, h * <sh> + kh, w * <sw> + kw) where kh in 0:<kH>, kw in 0:<kW>\n",
    "}\n",
    "\"\"\"\n",
    "sH, sW, kH, kW = 1, 1, 2, 2\n",
    "LANG = re.sub('<sh>', str(sH), LANG)\n",
    "LANG = re.sub('<sw>', str(sW), LANG)\n",
    "LANG = re.sub('<kH>', str(kH), LANG)\n",
    "LANG = re.sub('<kW>', str(kW), LANG)\n",
    "avgpool = tc.define(LANG, name=\"avgpool\")\n",
    "inp = torch.ones(1, 1, 4, 4).cuda()\n",
    "out5 = avgpool(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually injecting external CUDA code\n",
    "\n",
    "If you have an external efficient CUDA code that you want to use rather than the CUDA code that TC generates, you can inject your code easily. For this, you need to create a string which has the CUDA code you want to inject and you need to pass the name of the kernel and the CUDA code string to the `tc.define` call.\n",
    "\n",
    "As an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: No mapping options passed, 'naive' type mapping options will be used and will likely have bad performance. See help(your_layer.__call__) for setting mapping options.\n"
     ]
    }
   ],
   "source": [
    "lang = \"\"\"\n",
    "def add(float(N) A, float(N) B) -> (output) {\n",
    "    output(i) = A(i) + B(i) + 1\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cuda_code = \"\"\"\n",
    "extern \"C\"{\n",
    "__global__ void my_add(float* __restrict__ output, const float* __restrict__ A, const float* __restrict B)\n",
    "{\n",
    "    int t = threadIdx.x;\n",
    "    output[t] = A[t] + B[t];\n",
    "}\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "add = tc.define(lang, name=\"add\", inject_kernel=\"my_add\", cuda_code=cuda_code)\n",
    "a, b = torch.randn(100).cuda(), torch.randn(100).cuda()\n",
    "out6 = add(a, b, grid=[1, 1, 1], block=[100, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.2441\n",
       "-1.2044\n",
       "-0.1484\n",
       " 1.1395\n",
       " 0.2219\n",
       "-0.7885\n",
       " 0.3211\n",
       " 1.0028\n",
       "-0.7377\n",
       "-0.7845\n",
       " 2.4702\n",
       " 3.2829\n",
       " 0.7614\n",
       " 0.1627\n",
       " 1.4155\n",
       "-1.0388\n",
       " 1.3907\n",
       " 1.8067\n",
       "-0.3192\n",
       " 1.8688\n",
       " 0.0137\n",
       " 0.5402\n",
       " 0.4248\n",
       " 0.3889\n",
       "-1.7622\n",
       "-0.3826\n",
       " 1.3375\n",
       " 2.4420\n",
       "-2.5455\n",
       "-0.5168\n",
       " 0.0186\n",
       "-0.4861\n",
       "-0.8110\n",
       "-1.5189\n",
       "-0.1110\n",
       " 2.0514\n",
       "-0.4572\n",
       " 0.5803\n",
       "-0.9403\n",
       " 0.3659\n",
       "-2.4906\n",
       " 0.7857\n",
       " 0.0200\n",
       "-2.3367\n",
       " 1.1161\n",
       "-1.9790\n",
       " 1.1353\n",
       " 0.6458\n",
       " 0.5414\n",
       " 0.8869\n",
       " 2.6098\n",
       " 1.2731\n",
       " 1.7311\n",
       "-1.2708\n",
       " 3.0092\n",
       "-3.0787\n",
       " 0.8363\n",
       "-0.4576\n",
       "-1.5630\n",
       " 0.4198\n",
       "-0.8549\n",
       " 1.6064\n",
       " 0.7856\n",
       "-0.0349\n",
       "-1.0281\n",
       "-0.0128\n",
       " 0.0209\n",
       "-2.3107\n",
       " 2.0514\n",
       " 1.4023\n",
       " 0.4369\n",
       "-0.9508\n",
       " 1.4871\n",
       " 0.1565\n",
       "-0.2754\n",
       " 0.2455\n",
       "-0.5572\n",
       " 1.8748\n",
       " 1.5466\n",
       "-1.5910\n",
       " 0.0043\n",
       "-2.0473\n",
       "-1.4289\n",
       " 1.4102\n",
       "-0.0120\n",
       " 0.5479\n",
       " 1.0860\n",
       " 0.5362\n",
       " 0.5234\n",
       " 0.3940\n",
       " 0.1579\n",
       " 2.1710\n",
       " 0.6930\n",
       " 2.3230\n",
       "-2.3188\n",
       " 0.3688\n",
       "-1.5531\n",
       " 0.0285\n",
       " 1.4020\n",
       " 0.6835\n",
       "[torch.cuda.FloatTensor of size 100 (GPU 0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Functions\n",
    "\n",
    "TC allows using some CUDA built-in functions as well when defining the TC language. During the execution, CUDA API will be called for those built-in functions. For example, let’s say we want to use `fmax` CUDA function in our TC language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG = \"\"\"\n",
    "def relu(float(B,M) I) -> (O1){\n",
    "  O1(b, m) = fmax(I(b, m), 0)\n",
    "}\n",
    "\"\"\"\n",
    "relu = tc.define(LANG, name=\"relu\")\n",
    "inp = torch.randn(100, 128).cuda()\n",
    "out7 = relu(inp, options=tc.Options(\"mlp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.4639  0.6699  0.0000  ...   0.0000  1.5628  0.0000\n",
       " 0.9489  0.0000  1.1602  ...   3.3303  0.0000  0.2466\n",
       " 0.5919  1.5336  0.0000  ...   0.0000  0.0000  0.0000\n",
       "          ...             ⋱             ...          \n",
       " 1.0246  1.0487  0.0000  ...   1.5216  0.0021  0.4519\n",
       " 0.0000  0.0000  1.5521  ...   0.4257  0.0000  0.0000\n",
       " 0.0000  0.0727  0.0000  ...   0.0000  0.0000  0.3307\n",
       "[torch.cuda.FloatTensor of size 100x128 (GPU 0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### For more CUDA Math, go here \n",
    "\n",
    "[CUDA Math Doc](http://docs.nvidia.com/cuda/cuda-math-api/group__CUDA__MATH__SINGLE.html#group__CUDA__MATH__SINGLE)\n",
    "\n",
    "## Machine Learning Layers Database\n",
    "\n",
    "There is a database of about 30 machine learning layers that are used across various types of neural networks. \n",
    "\n",
    "If you want to use one of the layers in the database, you can query this database in your code easily. The database can be accessed by calling tc.database. This database is a dictionary of TC name to the TC definition. Each entry in the dictionary looks like: `{tc_name: {\"lang\": language, \"grad\": grad_language}}` where `tc_name` is the name of the operation, `lang` is the tc language describing that operation, `grad` is the TC language describing the gradient of that operation. The `grad` is optional entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have computed the matmul for mat1 and mat2 \n",
    "\n",
    "### Pooling Layers \n",
    "\n",
    "#### Average Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"\"\"\n",
    "        def avgpool(float(B, C, H, W) input) -> (output) {{\n",
    "            output(b, c, h, w) +=! input(b, c, h * {sH} + kh, w * {sW} + kw) / ({kH} * {kW}) where kh in 0:{kH}, kw in 0:{kW}\n",
    "        }}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG = \"\"\"\n",
    "def maxpool(float(B, C, H, W) input) -> (output) {{\n",
    "    output(b, c, h, w) max= input(b, c, h * {sH} + kh, w * {sW} + kw) where kh in 0:{kH}, kw in 0:{kW}\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already have computed the matmul for mat1 and mat2 \n",
    "\n",
    "### Convolutional Layers \n",
    "\n",
    "#### Simple Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = \"\"\"\n",
    "def convolution(float(N, C, H, W) I, float(M, C, KH, KW) W1, float(M) B) -> (O) {\n",
    "    O(n, m, h, w) +=! I(n, c, h + kh, w + kw) * W1(m, c, kh, kw)\n",
    "    O(n, m, h, w) = O(n, m, h, w) + B(m)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strided Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANG = \"\"\"\n",
    "def convolution_strided(float(N, C, H, W) I, float(M, C, KH, KW) W1, float(M) B) -> (O) {{\n",
    "    O(n, m, h, w) +=! I(n, c, {sh} * h + kh, {sw} * w + kw) * W1(m, c, kh, kw)\n",
    "    O(n, m, h, w) = O(n, m, h, w) + B(m)\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strided Conv Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "def convolution_grad(float(N, C, H, W) I, float(M, C, KH, KW) W1, float(N, M, H, W) O_grad) -> (I_grad, W1_grad) {{\n",
    "    I_grad(n, c, h, w) +=! O_grad(n, m, {sh} * h - kh, {sw} * w - kw) * W1(m, c, kh, kw)\n",
    "    W1_grad(m, c, kh, kw) +=! O_grad(n, m, {sh} * h - kh, {sw} * w - kw) * I(n, c, h, w)\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Group Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "\n",
    "def group_convolution(float(N, G, C, H, W) I, float(G, F, C, KH, KW) W1, float(G, F) B) -> (O) {\n",
    "    O(n, g, f, h, w) +=! I(n, g, c, h + kh, w + kw) * W1(g, f, c, kh, kw)\n",
    "    O(n, g, f, h, w) = O(n, g, f, h, w) + B(g, f)\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Group Conv. Strided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lang = \"\"\"\n",
    "def group_convolution_strided(float(N, G, C, H, W) I, float(G, F, C, KH, KW) W1, float(G, F) B) -> (O) {{\n",
    "    O(n, g, f, h, w) +=! I(n, g, c, {sh} * h + kh, {sw} * w + kw) * W1(g, f, c, kh, kw)\n",
    "    O(n, g, f, h, w) = O(n, g, f, h, w) + B(g, f)\n",
    "}}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
